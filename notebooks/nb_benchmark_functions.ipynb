{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8606a7eb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 13:28:12,158 - kedro.extras.extensions.ipython - INFO - Updated path to Kedro project: /home/aws_baustro_bi02/risk-stream\n",
      "2023-08-25 13:28:14,312 - kedro.framework.session.store - INFO - `read()` not implemented for `SQLiteStore`. Assuming empty store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: fs.s3a.aws.credentials.provider\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/aws_baustro_bi02/env_dev/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/aws_baustro_bi02/env_dev/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/aws_baustro_bi02/.ivy2/cache\n",
      "The jars for the packages stored in: /home/aws_baustro_bi02/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.crealytics#spark-excel_2.12 added as a dependency\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6aa1cd56-3010-4895-b871-dd2cbc34c882;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      "\tfound com.crealytics#spark-excel_2.12;0.13.1 in central\n",
      "\tfound org.apache.poi#poi;4.1.0 in central\n",
      "\tfound commons-codec#commons-codec;1.12 in central\n",
      "\tfound org.apache.commons#commons-collections4;4.3 in central\n",
      "\tfound org.apache.commons#commons-math3;3.6.1 in central\n",
      "\tfound org.apache.poi#poi-ooxml;4.1.0 in central\n",
      "\tfound org.apache.poi#poi-ooxml-schemas;4.1.0 in central\n",
      "\tfound org.apache.xmlbeans#xmlbeans;3.1.0 in central\n",
      "\tfound com.github.virtuald#curvesapi;1.06 in central\n",
      "\tfound com.norbitltd#spoiwo_2.12;1.6.0 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.12;1.2.0 in central\n",
      "\tfound org.joda#joda-convert;2.0.1 in central\n",
      "\tfound com.monitorjbl#xlsx-streamer;2.1.0 in central\n",
      "\tfound com.rackspace.apache#xerces2-xsd11;2.11.1 in central\n",
      "\tfound com.rackspace.eclipse.webtools.sourceediting#org.eclipse.wst.xml.xpath2.processor;2.1.100 in central\n",
      "\tfound edu.princeton.cup#java-cup;10k in central\n",
      "\tfound com.ibm.icu#icu4j;4.6 in central\n",
      "\tfound xml-resolver#xml-resolver;1.2 in central\n",
      "\tfound xml-apis#xml-apis;1.4.01 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      "\tfound org.apache.commons#commons-compress;1.19 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.8.8 in central\n",
      "\tfound com.amazon.deequ#deequ;2.0.0-spark-3.1 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sourceforge.f2j#arpack_combined_all;0.1 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      ":: resolution report :: resolve 539ms :: artifacts dl 19ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.0-spark-3.1 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.crealytics#spark-excel_2.12;0.13.1 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.8.8 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tcom.github.virtuald#curvesapi;1.06 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;4.6 from central in [default]\n",
      "\tcom.monitorjbl#xlsx-streamer;2.1.0 from central in [default]\n",
      "\tcom.norbitltd#spoiwo_2.12;1.6.0 from central in [default]\n",
      "\tcom.rackspace.apache#xerces2-xsd11;2.11.1 from central in [default]\n",
      "\tcom.rackspace.eclipse.webtools.sourceediting#org.eclipse.wst.xml.xpath2.processor;2.1.100 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.12 from central in [default]\n",
      "\tedu.princeton.cup#java-cup;10k from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\tnet.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n",
      "\torg.apache.commons#commons-collections4;4.3 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.19 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.6.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\torg.apache.poi#poi;4.1.0 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.1.0 from central in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.1.0 from central in [default]\n",
      "\torg.apache.xmlbeans#xmlbeans;3.1.0 from central in [default]\n",
      "\torg.joda#joda-convert;2.0.1 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.12;1.2.0 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\txml-apis#xml-apis;1.4.01 from central in [default]\n",
      "\txml-resolver#xml-resolver;1.2 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.commons#commons-compress;1.18 by [org.apache.commons#commons-compress;1.19] in [default]\n",
      "\torg.apache.poi#poi;4.0.0 by [org.apache.poi#poi;4.1.0] in [default]\n",
      "\torg.apache.poi#poi-ooxml;4.0.0 by [org.apache.poi#poi-ooxml;4.1.0] in [default]\n",
      "\torg.apache.poi#poi-ooxml-schemas;4.0.0 by [org.apache.poi#poi-ooxml-schemas;4.1.0] in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 by [org.apache.commons#commons-math3;3.6.1] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 by [org.slf4j#slf4j-api;1.7.25] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   45  |   0   |   0   |   7   ||   39  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6aa1cd56-3010-4895-b871-dd2cbc34c882\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 39 already retrieved (0kB/14ms)\n",
      "23/08/25 13:28:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/25 13:28:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/08/25 13:28:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/08/25 13:28:18 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/08/25 13:28:18 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/08/25 13:28:18 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/08/25 13:28:18 WARN FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.\n",
      "/home/aws_baustro_bi02/env_dev/lib/python3.8/site-packages/hdfs/config.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import load_source\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 13:28:24,442 - botocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2023-08-25 13:28:24,573 - kedro.extras.extensions.ipython - INFO - ** Kedro project risk-stream\n",
      "2023-08-25 13:28:24,574 - kedro.extras.extensions.ipython - INFO - Defined global variable `context`, `session`, `catalog` and `pipelines`\n",
      "2023-08-25 13:28:26,321 - kedro.extras.extensions.ipython - INFO - Registered line magic `run_viz`\n"
     ]
    }
   ],
   "source": [
    "# %load_ext kedro.extras.extensions.ipython\n",
    "%reload_ext kedro.extras.extensions.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "036c079d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Aug 25 01:28:28 PM: Encountered unexpected exception importing solver GLOP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.6.2534). Expected < 9.5.0.Please open a feature request on cvxpy to enable support for this version.')\n",
      "(CVXPY) Aug 25 01:28:28 PM: Encountered unexpected exception importing solver PDLP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.6.2534). Expected < 9.5.0.Please open a feature request on cvxpy to enable support for this version.')\n"
     ]
    }
   ],
   "source": [
    "# display all columns\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# mute warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# df shape\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "pd.set_option('display.width', 10000)\n",
    "\n",
    "# spark DF\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "# funciones\n",
    "from risk_stream.utils.commons import (get_last_register, \n",
    "                                       add_prefix_in_columns, \n",
    "                                       _create_yyyy_mm, \n",
    "                                       add_date_given_month_year, \n",
    "                                       cast_dataframe_columns, \n",
    "                                       dates_window_given_datecut\n",
    "                                      )\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import f_oneway, kruskal\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "import risk_stream.utils.eda as eda\n",
    "\n",
    "from risk_stream.utils.credit_scoring import tabla_resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45dc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def _fill_na_none(df, value, subset=None):\n",
    "    if subset is None:\n",
    "        columns_to_fill = df.columns\n",
    "    else:\n",
    "        columns_to_fill = subset\n",
    "    \n",
    "    df = (reduce(lambda reduce_df, \n",
    "                        col_name: (reduce_df\n",
    "                                   .withColumn(col_name,\n",
    "                                               F.when(F.col(col_name).isNull(), value)\n",
    "                                               .otherwise(F.col(col_name))\n",
    "                                              )\n",
    "                                  ),\n",
    "                        columns_to_fill,\n",
    "                        df\n",
    "                       )\n",
    "                )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9387fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "# notacion cientifica\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "# spark df\n",
    "real_estimado_pd = pd.DataFrame({\"real\": y_test, \"pred\": clf.predict(X_test)})\n",
    "activo = spark.createDataFrame(activo)\n",
    "\n",
    "# write, read\n",
    "leads_nptb_1 = pd.read_csv('leads_nptb_1.csv', sep='|', encoding = \"ISO-8859-1\")\n",
    "df_pivot_zero_pd.to_csv('resultados_score_no_ceros_campaigns_AA.csv', index=False, sep='|', decimal='.', encoding='utf-8-sig')\n",
    "\n",
    "Resultado_Cedulas_Producto_1 = pd.read_excel('Resultado_Cedulas_Producto_1.xlsx')\n",
    "\n",
    "# prefix, fillna\n",
    "ingresos_validados_productosBdA = ingresos_validados_productosBdA.fillna(0, subset = variables_clienteBDA)\n",
    "\n",
    "fea_cliente = add_prefix_in_columns(fea_cliente, \"demo_\", exclude=key_client_columns)\n",
    "\n",
    "# ~\n",
    "\n",
    "buro_var_resumen = [s for s in prm_buro.columns if buro_vars_resumen[0][2::1] in s]\n",
    "_data = (prm_buro\n",
    "         .withColumn(buro_vars_resumen[0], sum([F.col(c) for c in buro_var_resumen]))\n",
    "        )\n",
    "for i in range(1, len(buro_vars_resumen)):\n",
    "    buro_var_resumen = [s for s in prm_buro.columns if buro_vars_resumen[i][2::1] in s]\n",
    "    _data = (_data\n",
    "             .withColumn(buro_vars_resumen[i], sum([F.col(c) for c in buro_var_resumen]))\n",
    "            )\n",
    "\n",
    "_data = (_data\n",
    "         .select([s for s in _data.columns if 'b_' in s] + [\"fecha_observacion\", \"marca_bancarizado\"])\n",
    "        )\n",
    "\n",
    "# plt axis\n",
    "plt.xlabel(\"propensión\")\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "# barplot\n",
    "variable = cat_var[0]\n",
    "data = model_018_v0_data\n",
    "\n",
    "pd_barplot = data[variable].value_counts().to_frame()\n",
    "pd_barplot.reset_index(inplace=True)\n",
    "pd_barplot = pd_barplot.rename(columns={variable: \"count\", \"index\": variable})\n",
    "pd_barplot.plot.bar(x=variable, y='count', rot=0, figsize=(9,3), legend=False)\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "# Guardar en S3\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import yaml\n",
    "\n",
    "def _read_s3_file(user, path):\n",
    "    credentials = yaml.safe_load(open(fr\"/home/{user}/risk-stream/conf/local/credentials.yml\",'r'))\n",
    "    accessKeyId = credentials['dev_s3'].get('key')\n",
    "    secretAccessKey = credentials['dev_s3'].get('secret') \n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "                .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:3.1.1\") \\\n",
    "                .appName(\"app_name\") \\\n",
    "                .getOrCreate() \n",
    "\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.awsAccessKeyId\", accessKeyId)\n",
    "    spark._jsc.hadoopConfiguration().set(\"fs.s3a.awsSecretAccessKey\", secretAccessKey)\n",
    "\n",
    "    df = pd.read_csv(path, sep='|', encoding = \"ISO-8859-1\")\n",
    "    return df\n",
    "\n",
    "# df.toPandas().to_csv('s3://dataanalysis-austrodata/EXPERIMENTS/data_filtros_analisis.csv', index=False, sep='|', decimal='.', encoding='utf-8-sig')\n",
    "\n",
    "user = \"aws_baustro_bi02\"\n",
    "path = 's3://dataanalysis-austrodata/EXPERIMENTS/MEI_Perfiles_20230919.csv.bz2'\n",
    "\n",
    "_data = _read_s3_file(user, path)\n",
    "\n",
    "# write, read parquet\n",
    "leads_master_mei.write.parquet(\"master_indep_1_mei_v4.parquet\")\n",
    "spark.read.parquet(\"master_indep_1_mei_v4.parquet\")\n",
    "\n",
    "# data_filtros_analisis:\n",
    "#   <<: *csv_dataset\n",
    "#   filepath: ${base_path_pandas}/EXPERIMENTS/data_filtros_analisis.csv\n",
    "#   load_args:\n",
    "#     delimiter: \"|\"\n",
    "#     encoding: \"utf-8-sig\"\n",
    "\n",
    "# eda\n",
    "eda.null_function(model_018_v0_data[num_var + cat_var + [\"target\"]])\n",
    "\n",
    "df_eda_num = eda.numeric_eda(model_018_v0_data, num_var)\n",
    "df_eda_num\n",
    "\n",
    "df_eda_cat = eda.category_eda(model_018_v0_data, cat_var)\n",
    "df_eda_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180bb91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508fbfd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## commons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da9e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import logging\n",
    "import re\n",
    "import unicodedata\n",
    "from functools import reduce\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame, functions as F, Window\n",
    "\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, log_loss\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "# import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "days = lambda i: i * 86400\n",
    "\n",
    "def _mdt_to_production_dev(model):\n",
    "    # user = getpass.getuser()\n",
    "    \n",
    "    # os.chdir('../../..')\n",
    "    current_working_directory = os.getcwd()\n",
    "    current_working_directory_final = current_working_directory + '/conf/base/parameters.yml'\n",
    "    \n",
    "    # print(current_working_directory_final)\n",
    "    # breakpoint()\n",
    "    \n",
    "    with open(current_working_directory_final,'r') as f:\n",
    "        parametros = yaml.safe_load(f)\n",
    "    \n",
    "    \n",
    "\n",
    "    production = parametros.get(\"campanias\")['production']\n",
    "    \n",
    "\n",
    "    if production:\n",
    "        output = f\"mdt_campania_{model}\"\n",
    "    else:\n",
    "        output = f\"mdt_campania_{model}_local\"\n",
    "    return output\n",
    "\n",
    "def clean_pandas_column_names(pandas_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Refactor pandas DataFrame column names including lower casing,\n",
    "    and replacement of non-alpha characters with underscores or words.\n",
    "    Args:\n",
    "        pandas_df: a dataframe\n",
    "g\n",
    "    Returns:\n",
    "        pd.DataFrame with columns names in low\n",
    "    \"\"\"\n",
    "    return pandas_df.rename(columns=clean_column_name)\n",
    "\n",
    "\n",
    "def rename_duplicate_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Rename column headers to ensure no header names are duplicated.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A dataframe with a single index of columns\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with headers renamed; inplace\n",
    "    \"\"\"\n",
    "    if not df.columns.has_duplicates:\n",
    "        return df\n",
    "    duplicates: Set[str] = set(df.columns[df.columns.duplicated()].tolist())\n",
    "    indexes: Dict[str, int] = defaultdict(lambda: 0)\n",
    "    new_cols: List[str] = []\n",
    "    for col in df.columns:\n",
    "        if col in duplicates:\n",
    "            indexes[col] += 1\n",
    "            new_cols.append(f\"{col}.{indexes[col]}\")\n",
    "        else:\n",
    "            new_cols.append(col)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "\n",
    "\n",
    "def rename_columns(\n",
    "        _data: DataFrame, dictionary: str = None, columns: Dict[str, str] = None\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "        Rename columns\n",
    "    Args:\n",
    "        columns:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    params = columns\n",
    "    if dictionary is not None:\n",
    "        params = get_config_dictionary(dictionary)\n",
    "\n",
    "    for old_name, new_name in params.items():\n",
    "        _data = _data.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def remove_accents(value: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove accents NON-ASCII character.\n",
    "    Args:\n",
    "        value: str -> Text\n",
    "    Returns:\n",
    "        str: Result\n",
    "    \"\"\"\n",
    "    if value == None:\n",
    "        return value\n",
    "    import unicodedata\n",
    "\n",
    "    value = unicodedata.normalize(\"NFD\", value)\n",
    "    value = value.encode(\"ascii\", \"ignore\")\n",
    "    value = value.decode(\"utf-8\")\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def clean_column_name(column_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Refactor pandas column names including lower casing,\n",
    "    and replacement of non-alpha characters with underscores or words.\n",
    "\n",
    "    Args:\n",
    "        column_name(str): a 'dirty' column name\n",
    "    Returns:\n",
    "        column_name(str): a 'clean' column name\n",
    "    \"\"\"\n",
    "    _old_column_name = str(column_name)\n",
    "    column_new = inflection.underscore(str(column_name).strip())\n",
    "    column_new = (\n",
    "        unicodedata.normalize(\"NFKD\", column_new)\n",
    "            .encode(\"ascii\", \"ignore\")\n",
    "            .decode(\"utf-8\")\n",
    "    )\n",
    "\n",
    "    column_new = column_new.replace(\"(%)\", \"_percent\")\n",
    "    column_new = re.sub(\"[ :_\\\\.,;{}()'\\n\\t=]+\", \"_\", column_new)\n",
    "    column_new = re.sub(\"%\", \"percent\", column_new)\n",
    "    column_new = column_new.replace(\"?\", \"question\")\n",
    "    column_new = re.sub(\"[-]+\", \"minus\", column_new)\n",
    "    column_new = re.sub(\"[/]+\", \"by\", column_new)\n",
    "    column_new = re.sub(\"#\", \"number\", column_new)\n",
    "    column_new = re.sub(\"[&+]+\", \"and\", column_new)\n",
    "    column_new = re.sub(\"[|,;]+\", \"or\", column_new)\n",
    "    logger.debug(\"%s - %s\", _old_column_name, column_new)\n",
    "\n",
    "    return column_new\n",
    "\n",
    "\n",
    "def clean_spark_column_names(pyspark_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Refactor pyspark DataFrame column names including lower casing,\n",
    "    and replacement of non-alpha characters with underscores or words.\n",
    "    Args:\n",
    "        pandas_df: a dataframe\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame with columns names in low\n",
    "    \"\"\"\n",
    "    for x in pyspark_df.columns:\n",
    "        pyspark_df = pyspark_df.withColumnRenamed(x, clean_column_name(x))\n",
    "    return pyspark_df\n",
    "\n",
    "\n",
    "def trim_all_string_columns(_data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Remove spaces between strings\n",
    "    Args:\n",
    "        data: DataFrame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    for column, types in _data.dtypes:\n",
    "        if types == \"string\":\n",
    "            _data = _data.withColumn(column, F.trim(F.col(column)))\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def convert_columns_double_to_float(_data: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert all double columns to float\n",
    "    Args:\n",
    "        data:DataFrame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "\n",
    "    \"\"\"\n",
    "    for column, types in _data.dtypes:\n",
    "        if types in [\"double\", \"decimal\"]:\n",
    "            _data = _data.withColumn(column, F.col(column).cast(\"float\"))\n",
    "\n",
    "    return _data\n",
    "\n",
    "\n",
    "def extract_date_by_partitioned_dataframe(partitioned_input: Dict, prefix_file: str, extract, date_format: str = \"yyyy-MM-dd\" ):\n",
    "    \"\"\"\n",
    "    Extract from filnanme date and return DataFrame\n",
    "    Args:\n",
    "        date_format: Default \"yyyy-MM-dd\"\n",
    "        partitioned_input: Partitioned DataFrame\n",
    "        prefix_file: Prefix file\n",
    "        extract: Function to extract correct date\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    lst_data = []\n",
    "    for partition_key, partition_dataframe in partitioned_input.items():\n",
    "        if prefix_file in partition_key:\n",
    "            date_file = extract(partition_key)\n",
    "            tmp_df = partition_dataframe()\n",
    "            tmp_df = tmp_df.withColumn(\"fecha_filename\", F.to_date(F.lit(date_file), date_format))\n",
    "            lst_data.append(tmp_df)\n",
    "\n",
    "    # Concat all dataframe\n",
    "    _data = reduce(DataFrame.unionAll, lst_data)\n",
    "    return _data\n",
    "\n",
    "def add_prefix_in_columns(data: DataFrame, prefix: str, exclude: List = [], is_prefix=True) -> DataFrame:\n",
    "    all_columns = data.columns\n",
    "\n",
    "    alias_columns = []\n",
    "    for column in all_columns:\n",
    "        col = F.col(column)\n",
    "        if column not in exclude:\n",
    "            if is_prefix == True:\n",
    "                col = col.alias(prefix + str(column))\n",
    "            else:\n",
    "                col = col.alias(str(column) + prefix)\n",
    "        alias_columns.append(col)\n",
    "\n",
    "    return data.select(*alias_columns)\n",
    "\n",
    "\n",
    "\n",
    "def filter_invalid_codigo_cliente(data: DataFrame, column: str = \"codigo_cliente\") -> DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    regex_codigo_cliente = '(?i)^[a-z]{2}[0-9]{6}$'\n",
    "\n",
    "    # Remove invalid codigo_cliente\n",
    "    return data.filter(F.col(column).rlike(regex_codigo_cliente) == F.lit(True)).filter(f\"{column} is not null\")\n",
    "\n",
    "\n",
    "def get_last_register(df: DataFrame, window_partition_by: List, order_by: str, last_register: int = 1) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create dataframe partitioned by columns and orderby fecha\n",
    "    Args:\n",
    "        df:\n",
    "        window_partition_by:\n",
    "        order_by:\n",
    "        last_register:\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    w = (Window\n",
    "         .partitionBy([F.col(x) for x in window_partition_by])\n",
    "         .orderBy(F.desc(order_by))\n",
    "         )\n",
    "\n",
    "    df = (df\n",
    "          .withColumn('rank', F.dense_rank().over(w))\n",
    "          )\n",
    "\n",
    "    last_df = (df\n",
    "               .filter(F.col('rank') == last_register)\n",
    "               .drop(F.col('rank'))\n",
    "               )\n",
    "\n",
    "    return last_df\n",
    "\n",
    "\n",
    "def cast_dataframe_columns(dataframe: DataFrame, columns: List, convert_to: str):\n",
    "    \"\"\"\n",
    "    Cast list of columns in dataframe\n",
    "    Args:\n",
    "        dataframe: DataFrame\n",
    "        columns: List of columns\n",
    "        convert_to: Type\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        dataframe = dataframe.withColumn(column, F.col(column).cast(convert_to))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def cast_columns_date(dataframe: DataFrame, columns: List, FORMAT_DATE: str, convert_to: str = 'date'):\n",
    "    \"\"\"\n",
    "    Cast list of columns in dataframe\n",
    "    Args:\n",
    "        dataframe: DataFrame\n",
    "        columns: List of columns\n",
    "        convert_to: Type\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        dataframe = dataframe.withColumn(column, F.date_format(F.col(column),FORMAT_DATE).cast(convert_to))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "def standardize_null_columns(dataframe: DataFrame, columns: List):\n",
    "    \"\"\"\n",
    "    Standardize Nulls of columns in dataframe\n",
    "    Args:\n",
    "        dataframe: DataFrame\n",
    "        columns: List of columns\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    for column in columns:\n",
    "        dataframe = (dataframe\n",
    "                     .withColumn(column,F.when(F.col(column) == F.lit(\"NA\"), None)\n",
    "                                 .when(F.col(column) == F.lit(\"NULL\"), None)\n",
    "                                 .when(F.col(column) == F.lit(\"null\"), None)\n",
    "                                 .when(F.col(column) == F.lit(\"#N/A\"), None)\n",
    "                                 .when(F.col(column) == F.lit(\"(null)\"), None)\n",
    "                                 .when(F.col(column) == F.lit(\"ERROR:#N/A\"), None)\n",
    "                                 .when(F.col(column) == F.lit(\"ERROR:#REF!\"), None)\n",
    "                                 .when(F.col(column) == F.lit('\"\"\"\"'), None)\n",
    "                                 .otherwise(F.col(column))\n",
    "                                )\n",
    "                    )\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def remove_incorrect_date(_data: DataFrame, column: str, FORMAT_DATE:str = \"yyyy/MM/dd\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Changes to the due date when the date is greater or less than allowed in parquet files.\n",
    "\n",
    "    Args:\n",
    "        _data: DataFrame\n",
    "        column: column name\n",
    "        FORMAT_DATE: Format\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    return (_data.withColumn(column,\n",
    "                             F.to_date(F.col(column), FORMAT_DATE))\n",
    "                  .withColumn(column, F.when(\n",
    "                        F.col(column).between(F.to_timestamp(F.lit(str(pd.Timestamp.min))), F.to_timestamp(F.lit(str(pd.Timestamp.max)))\n",
    "                ), F.col(column)))\n",
    "            )\n",
    "\n",
    "def get_group_variable(X, X_train, X_test, parameters):\n",
    "    \n",
    "    grouping_variables = parameters.get(\"grouping_variables\", None)\n",
    "    cum_percentage = parameters.get(\"cum_percentage\", None)\n",
    "    \n",
    "    for categoria in grouping_variables:\n",
    "\n",
    "        X, X_train, X_test = group_variable(X, X_train, X_test, categoria, cum_percentage)\n",
    "        \n",
    "    X = X.drop(columns=grouping_variables)    \n",
    "    X_train = X_train.drop(columns=grouping_variables)\n",
    "    X_test = X_test.drop(columns=grouping_variables)\n",
    "    \n",
    "    col_list = list(set().union(X_train.columns, X_test.columns))\n",
    "    \n",
    "    X_train = X_train.reindex(columns=col_list, fill_value=0)\n",
    "    X_train = X_train[col_list]\n",
    "    \n",
    "    X_test = X_test.reindex(columns=col_list, fill_value=0)\n",
    "    X_test = X_test[col_list]\n",
    "    \n",
    "    return [X, X_train, X_test]\n",
    "\n",
    "def get_group_variable_predict(X_train, X_predict, parameters):\n",
    "    \n",
    "    X_train = X_train.toPandas()\n",
    "    X_predict = X_predict.toPandas()\n",
    "    \n",
    "    grouping_variables = parameters.get(\"grouping_variables\", None)\n",
    "    cum_percentage = parameters.get(\"cum_percentage\", None)\n",
    "    \n",
    "    for categoria in grouping_variables:\n",
    "\n",
    "        X_train, X_predict = group_variable_predict(X_train, X_predict, categoria, cum_percentage)\n",
    "        \n",
    "    X_train = X_train.drop(columns=grouping_variables)    \n",
    "    X_predict = X_predict.drop(columns=grouping_variables)\n",
    "    \n",
    "    col_list = list(set().union(X_train.columns, X_predict.columns))\n",
    "    \n",
    "    X_train = X_train.reindex(columns=col_list, fill_value=0)\n",
    "    X_train = X_train[col_list]\n",
    "    \n",
    "    X_predict = X_predict.reindex(columns=col_list, fill_value=0)\n",
    "    X_predict = X_predict[col_list]\n",
    "\n",
    "    return [X_train, X_predict]\n",
    "        \n",
    "def group_variable(X, X_train, X_test, categoria, cum_percentage):\n",
    "    \n",
    "    df_agg_X = (X.groupby([categoria]).agg({'codigo_cliente':['count']}))\n",
    "    \n",
    "    df_agg = (X_train.groupby([categoria]).agg({'codigo_cliente':['count']}))\n",
    "    \n",
    "    df_agg_X.columns = df_agg_X.columns.droplevel(0)\n",
    "    \n",
    "    df_agg.columns = df_agg.columns.droplevel(0)\n",
    "    \n",
    "    df_agg_X = df_agg_X.sort_values('count', ascending=False)\n",
    "\n",
    "    df_agg = df_agg.sort_values('count', ascending=False)\n",
    "    \n",
    "    df_agg_X[f'{categoria}_cum_percent'] = (df_agg_X['count'].cumsum() / df_agg_X['count'].sum())\n",
    "\n",
    "    df_agg[f'{categoria}_cum_percent'] = (df_agg['count'].cumsum() / df_agg['count'].sum())\n",
    "    \n",
    "    df_filter_X = df_agg_X[df_agg_X[f'{categoria}_cum_percent'] < cum_percentage]\n",
    "\n",
    "    df_filter = df_agg[df_agg[f'{categoria}_cum_percent'] < cum_percentage]\n",
    "    \n",
    "    if df_filter_X.shape[0] == 0:\n",
    "        df_filter_X = df_agg_X.iloc[:1]\n",
    "    \n",
    "    if df_filter.shape[0] == 0:\n",
    "        df_filter = df_agg.iloc[:1]\n",
    "        \n",
    "    X[f'{categoria}_group'] = X.apply(lambda x: x[categoria] if x[categoria] \n",
    "                               in df_filter_X.index.values else 'others', axis = 1)\n",
    "    \n",
    "    X_train[f'{categoria}_group'] = X_train.apply(lambda x: x[categoria] if x[categoria] \n",
    "                               in df_filter.index.values else 'others', axis = 1)\n",
    "    \n",
    "    X_test[f'{categoria}_group'] = X_test.apply(lambda x: x[categoria] if x[categoria] \n",
    "                               in df_filter.index.values else 'others', axis = 1)\n",
    "    \n",
    "    return [X, X_train, X_test]\n",
    "\n",
    "def group_variable_predict(X_train, X_predict, categoria, cum_percentage):\n",
    "    \n",
    "    df_agg_X_train = (X_train.groupby([categoria]).agg({'codigo_cliente':['count']}))\n",
    "    \n",
    "    df_agg_X_train.columns = df_agg_X_train.columns.droplevel(0)\n",
    "    \n",
    "    df_agg_X_train = df_agg_X_train.sort_values('count', ascending=False)\n",
    "    \n",
    "    df_agg_X_train[f'{categoria}_cum_percent'] = (df_agg_X_train['count'].cumsum() / df_agg_X_train['count'].sum())\n",
    "    \n",
    "    df_filter_X_train = df_agg_X_train[df_agg_X_train[f'{categoria}_cum_percent'] < cum_percentage]\n",
    "    \n",
    "    if df_filter_X_train.shape[0] == 0:\n",
    "        df_filter_X = df_agg_X.iloc[:1]\n",
    "        \n",
    "    X_train[f'{categoria}_group'] = X_train.apply(lambda x: x[categoria] if x[categoria] \n",
    "                               in df_filter_X_train.index.values else 'others', axis = 1)\n",
    "    \n",
    "    X_predict[f'{categoria}_group'] = X_predict.apply(lambda x: x[categoria] if x[categoria] \n",
    "                               in df_filter_X_train.index.values else 'others', axis = 1)\n",
    "    \n",
    "    return [X_train, X_predict]\n",
    "    \n",
    "def get_dummies(df, categorical_list):\n",
    "\n",
    "    if categorical_list is None or categorical_list == []:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        cat_df = pd.DataFrame()\n",
    "        for cat in categorical_list:\n",
    "            df[cat] = df[cat].str.replace('[^\\x00-\\x7F]+', '_')\n",
    "            var_df = pd.get_dummies(df[cat], prefix=cat, drop_first = True)\n",
    "            cat_df = pd.concat([cat_df, var_df], axis=1)\n",
    "\n",
    "    cat_df = cat_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+|[(]|[)]', '', x))\n",
    "\n",
    "    return cat_df\n",
    "\n",
    "def dates_window_given_datecut(df: DataFrame, \n",
    "                               date_column: str, \n",
    "                               datecut:str = \"yyyy-MM-dd\", \n",
    "                               x_past:int = 0, \n",
    "                               x_future:int = 0, \n",
    "                               monthly:bool = True\n",
    "                              ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Filter a dataframe according to a date window, taking into account a cut-off date and a period of time in the future and in the past, the time period can be monthly or daily.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        date_column: str\n",
    "        datecut: Default \"yyyy-MM-dd\"\n",
    "        x_past: int\n",
    "        x_future: int\n",
    "        monthly:bool\n",
    "\n",
    "    Returns:\n",
    "        _df: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    _df = (df\n",
    "           .withColumn('datecut', F.lit(datecut))\n",
    "           )\n",
    "    if monthly:\n",
    "        if (x_past != 0) & (x_future != 0):\n",
    "            _df = (_df\n",
    "                   .withColumn('x_past', F.add_months(F.col(\"datecut\"), -x_past))\n",
    "                   .withColumn('x_future', F.add_months(F.col(\"datecut\"), x_future))\n",
    "                   .filter((F.col(\"x_past\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"x_future\")))\n",
    "                   .drop('x_past', 'x_future')\n",
    "                  )\n",
    "        if (x_past != 0) & (x_future == 0):\n",
    "            _df = (_df\n",
    "                   .withColumn('x_past', F.add_months(F.col(\"datecut\"), -x_past))\n",
    "                   .filter((F.col(\"x_past\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"datecut\")))\n",
    "                   .drop('x_past')\n",
    "                  )\n",
    "        if (x_past == 0) & (x_future != 0):\n",
    "            _df = (_df\n",
    "                   .withColumn('x_future', F.add_months(F.col(\"datecut\"), x_future))\n",
    "                   .filter((F.col(\"datecut\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"x_future\")))\n",
    "                   # .drop('x_future')\n",
    "                  )\n",
    "    else:\n",
    "        if (x_past != 0) & (x_future != 0):\n",
    "            _df = (_df\n",
    "                   .withColumn('x_past', F.date_sub(F.col(\"datecut\"), x_past))\n",
    "                   .withColumn('x_future', F.date_add(F.col(\"datecut\"), x_future))\n",
    "                   .filter((F.col(\"x_past\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"x_future\")))\n",
    "                   .drop('x_past', 'x_future')\n",
    "                  )\n",
    "        if (x_past != 0) & (x_future == 0):\n",
    "            _df = (_df\n",
    "                   .withColumn('x_past', F.date_sub(F.col(\"datecut\"), x_past))\n",
    "                   .filter((F.col(\"x_past\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"datecut\")))\n",
    "                   .drop('x_past')\n",
    "                  )\n",
    "        if (x_past == 0) & (x_future != 0):\n",
    "            _df = (_df\n",
    "                   .withColumn('x_future', F.date_add(F.col(\"datecut\"), x_future))\n",
    "                   .filter((F.col(\"datecut\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"x_future\")))\n",
    "                   .drop('x_future')\n",
    "                  )        \n",
    "    _df = (_df\n",
    "           .drop('datecut')\n",
    "           )\n",
    "    return _df\n",
    "\n",
    "def active_client_time_window(df: DataFrame,\n",
    "                              date_column: str,\n",
    "                              group_by: list,\n",
    "                              trx_by: str,\n",
    "                              datecut:str = \"yyyy-MM-dd\",\n",
    "                              x_past:int = 0,\n",
    "                              monthly:bool = True\n",
    "                             ) -> DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Filter a dataframe according to a date window, taking into account a cut-off date and a period of time in the future and in the past, the time period can be monthly or daily.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame,\n",
    "        date_column: str\n",
    "        group_by: list\n",
    "        trx_by: str\n",
    "        datecut: Default \"yyyy-MM-dd\"\n",
    "        x_past: int\n",
    "        monthly: bool\n",
    "\n",
    "    Returns:\n",
    "        _df: DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    _df = df\n",
    "    \n",
    "    if len(group_by) == 1:        \n",
    "        if monthly:\n",
    "            if (x_past != 0):\n",
    "                _df = (_df\n",
    "                       .withColumn('datecut', F.lit(datecut))\n",
    "                       .withColumn('x_past', F.add_months(F.col(\"datecut\"), -x_past))\n",
    "                       .filter((F.col(\"x_past\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"datecut\")))\n",
    "                       .groupBy(group_by)\n",
    "                       .agg(F.countDistinct(trx_by).alias(\"flag\"))                       \n",
    "                       .withColumn(\"flag_activo\", F.when(F.col(\"flag\") == 1, \"INACTIVO\").otherwise(\"ACTIVO\"))\n",
    "                       .drop(\"flag\", \"numero\")\n",
    "                      )\n",
    "        else:\n",
    "            if (x_past != 0):\n",
    "                _df = (_df\n",
    "                       .withColumn('datecut', F.lit(datecut))\n",
    "                       .withColumn('x_past', F.date_sub(F.col(\"datecut\"), x_past))\n",
    "                       .filter((F.col(\"x_past\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"datecut\")))\n",
    "                       .groupBy(group_by)\n",
    "                       .agg(F.countDistinct(trx_by).alias(\"flag\"))                       \n",
    "                       .withColumn(\"flag_activo\", F.when(F.col(\"flag\") == 1, \"INACTIVO\").otherwise(\"ACTIVO\"))\n",
    "                       .drop(\"flag\", \"numero\")\n",
    "                      )        \n",
    "    \n",
    "    else:    \n",
    "        if monthly:\n",
    "            if (x_past != 0):\n",
    "                _df = (_df\n",
    "                       .withColumn('datecut', F.lit(datecut))\n",
    "                       .withColumn('x_past', F.add_months(F.col(\"datecut\"), -x_past))\n",
    "                       .filter((F.col(\"x_past\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"datecut\")))\n",
    "                       .groupBy(group_by)\n",
    "                       .agg(F.countDistinct(trx_by).alias(\"flag\"))\n",
    "                       .groupBy(group_by[0])\n",
    "                       .agg(F.sum(\"flag\").alias(\"flag\"), \n",
    "                            F.count(group_by[1]).alias(\"numero\")\n",
    "                           )\n",
    "                       .withColumn(\"flag_activo\", F.when(F.col(\"flag\") == F.col(\"numero\"), \"INACTIVO\").otherwise(\"ACTIVO\"))\n",
    "                       .drop(\"flag\", \"numero\")\n",
    "                      )\n",
    "        else:\n",
    "            if (x_past != 0):\n",
    "                _df = (_df\n",
    "                       .withColumn('datecut', F.lit(datecut))\n",
    "                       .withColumn('x_past', F.date_sub(F.col(\"datecut\"), x_past))\n",
    "                       .filter((F.col(\"x_past\") <= F.col(date_column)) & (F.col(date_column) < F.col(\"datecut\")))\n",
    "                       .groupBy(group_by)\n",
    "                       .agg(F.countDistinct(trx_by).alias(\"flag\"))\n",
    "                       .groupBy(group_by[0])\n",
    "                       .agg(F.sum(\"flag\").alias(\"flag\"), \n",
    "                            F.count(group_by[1]).alias(\"numero\")\n",
    "                           )\n",
    "                       .withColumn(\"flag_activo\", F.when(F.col(\"flag\") == F.col(\"numero\"), \"INACTIVO\").otherwise(\"ACTIVO\"))\n",
    "                       .drop(\"flag\", \"numero\")\n",
    "                      )\n",
    "    return _df\n",
    "\n",
    "def _last_register_yyyy_mm(df: DataFrame,\n",
    "                           group_variables: list,\n",
    "                           date_column: str\n",
    "                          ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a dataframe with last records according to a group_variables list\n",
    "    Args:\n",
    "        df: Dataframe\n",
    "        group_variables: list\n",
    "        date_column: str\n",
    "    Returns:\n",
    "        last_register_yyyy_mm: DataFrame\n",
    "    \"\"\"\n",
    "    last_register_yyyy_mm = (df\n",
    "                             .withColumn(\"yyyy_snapshot\", F.date_format(date_column, \"yyyy\")) \n",
    "                             .withColumn(\"mm_snapshot\", F.date_format(date_column, \"MM\")) \n",
    "                            )\n",
    "    last_register_yyyy_mm = get_last_register(last_register_yyyy_mm, group_variables, date_column)\n",
    "    return last_register_yyyy_mm\n",
    "\n",
    "def uplift_stat(y, yhat, at=0.3):\n",
    "    uplift = lift(y, yhat, at=at)\n",
    "    return uplift\n",
    "\n",
    "def lift(y, yhat, at):\n",
    "    \"\"\"Calculate lift.\"\"\"\n",
    "    # assert is_binary(y_true)\n",
    "    # assert not is_binary(y_pred_proba)\n",
    "    percentages, gains = cumulative_gain_curve(y, yhat, pos_label=1)\n",
    "    percentages = percentages[1:]\n",
    "    gains = gains[1:]\n",
    "    uplift = gains / percentages\n",
    "    n = len(gains)\n",
    "    uplift_at = uplift[int(n * at)]\n",
    "    return uplift_at\n",
    "\n",
    "\n",
    "def cumulative_gain_curve(\n",
    "    y_true, y_score, pos_label: str = None\n",
    ") -> Tuple[np.array, np.array]:\n",
    "    \"\"\"Generate the points necessary to plot the Cumulative Gain.\n",
    "    Note: This implementation is restricted to the binary classification task.\n",
    "    Args:\n",
    "        y_true (array-like, shape (n_samples)): True labels of the data.\n",
    "        y_score (array-like, shape (n_samples)): Target scores, can either be\n",
    "            probability estimates of the positive class, confidence values, or\n",
    "            non-thresholded measure of decisions (as returned by\n",
    "            decision_function on some classifiers).\n",
    "        pos_label (int or str, default=None): Label considered as positive and\n",
    "            others are considered negative\n",
    "    Returns:\n",
    "        percentages (numpy.ndarray): An array containing the X-axis values for\n",
    "            plotting the Cumulative Gains chart.\n",
    "        gains (numpy.ndarray): An array containing the Y-axis values for one\n",
    "            curve of the Cumulative Gains chart.\n",
    "    Raises:\n",
    "        ValueError: If `y_true` is not composed of 2 classes. The Cumulative\n",
    "            Gain Chart is only relevant in binary classification.\n",
    "    \"\"\"\n",
    "    y_true, y_score = np.asarray(y_true), np.asarray(y_score)\n",
    "    # ensure binary classification if pos_label is not specified\n",
    "    classes = np.unique(y_true)\n",
    "    if pos_label is None and not (\n",
    "        np.array_equal(classes, [0, 1])\n",
    "        or np.array_equal(classes, [-1, 1])\n",
    "        or np.array_equal(classes, [0])\n",
    "        or np.array_equal(classes, [-1])\n",
    "        or np.array_equal(classes, [1])\n",
    "    ):\n",
    "        raise ValueError(\"Data is not binary and pos_label is not specified\")\n",
    "    elif pos_label is None:\n",
    "        pos_label = 1.0\n",
    "    # make y_true a boolean vector\n",
    "    y_true = y_true == pos_label\n",
    "    sorted_indices = np.argsort(y_score)[::-1]\n",
    "    y_true = y_true[sorted_indices]\n",
    "    gains = np.cumsum(y_true)\n",
    "    percentages = np.arange(start=1, stop=len(y_true) + 1)\n",
    "    gains = gains / float(np.sum(y_true))\n",
    "    percentages = percentages / float(len(y_true))\n",
    "    gains = np.insert(gains, 0, [0])\n",
    "    percentages = np.insert(percentages, 0, [0])\n",
    "    return percentages, gains\n",
    "\n",
    "def add_date_given_month_year(df, \n",
    "                              month_column, \n",
    "                              year_column, \n",
    "                              new_date_column_name\n",
    "                             ) -> DataFrame:\n",
    "    _df = (df\n",
    "           .withColumn(month_column, F.format_string('%02d', month_column))\n",
    "           .withColumn(new_date_column_name,\n",
    "                       F.add_months(\n",
    "                           F.to_date(                     \n",
    "                               F.concat(F.col(year_column), \n",
    "                                        F.lit('-'), \n",
    "                                        F.col(month_column), \n",
    "                                        F.lit('-01')\n",
    "                                       ),\n",
    "                               \"yyyy-MM-dd\"\n",
    "                           ),\n",
    "                           1\n",
    "                       )\n",
    "                      )\n",
    "           .withColumn(new_date_column_name, F.date_sub(F.col(new_date_column_name), 1))\n",
    "          )\n",
    "    return _df\n",
    "\n",
    "def _create_yyyy_mm(df, \n",
    "                    date_col\n",
    "                   ) -> DataFrame:\n",
    "    create_yyyy_mm = (df\n",
    "                      .withColumn(\"yyyy_snapshot\", F.year(date_col))\n",
    "                      .withColumn(\"mm_snapshot\", F.month(date_col))\n",
    "                     )\n",
    "    return create_yyyy_mm\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def ks_stat(y, yhat):\n",
    "    return ks_2samp(yhat[y==1], yhat[y!=1]).statistic\n",
    "\n",
    "\n",
    "ks_scorer = make_scorer(ks_stat, needs_proba=True)\n",
    "\n",
    "roc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "log_scorer = make_scorer(log_loss, needs_proba=True, greater_is_better=False)\n",
    "\n",
    "lift_scorer = make_scorer(uplift_stat, needs_proba=True, greater_is_better=True)\n",
    "\n",
    "def get_rolling_average(df: DataFrame, \n",
    "                        window_partition_by: List, \n",
    "                        order_by: str, \n",
    "                        window_days: int,\n",
    "                        value: float,\n",
    "                        last_register: int = 1\n",
    "                       ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create dataframe partitioned by columns and orderby fecha\n",
    "    Args:\n",
    "        df:\n",
    "        window_partition_by:\n",
    "        order_by:\n",
    "        last_register:\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    df = (df \n",
    "          .withColumn('timestampGMT', \n",
    "                      F.col(order_by).astype('Timestamp'))\n",
    "          .withColumn('timestamp', \n",
    "                      F.col(\"timestampGMT\").cast('long')\n",
    "                     )\n",
    "         )\n",
    "    \n",
    "    w = (Window\n",
    "         .partitionBy([F.col(x) for x in window_partition_by])\n",
    "         .orderBy(F.col('timestamp'))\n",
    "         .rangeBetween(-days(window_days),0)\n",
    "        )\n",
    "\n",
    "    df = (df\n",
    "          .withColumn(f'rolling_average_{window_days}_days_{value}', \n",
    "                      F.mean(value)\n",
    "                      .over(w)\n",
    "                     )\n",
    "          .withColumn(f'rolling_average_{window_days}_days_{value}', F.col(f'rolling_average_{window_days}_days_{value}').cast(\"float\"))\n",
    "          .drop('timestampGMT', 'timestamp')\n",
    "         )\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_average_table(dataframe, column_name):\n",
    "    _data = (dataframe\n",
    "               .groupBy(column_name)\n",
    "               .agg(\n",
    "                   F.percentile_approx(\"iess_salario\", 0.5).alias(f'average_{column_name}'),\n",
    "                   F.count(\"iess_salario\").alias(f'count_{column_name}')\n",
    "               )\n",
    "              )\n",
    "    \n",
    "    return _data\n",
    "\n",
    "def feature_resumen_ingresos(prm_ingresos_iess: DataFrame, variables_list):\n",
    "    \n",
    "    for variable in variables_list:\n",
    "    \n",
    "        _data = get_average_table(prm_ingresos_iess, variable)\n",
    "        prm_ingresos_iess = (prm_ingresos_iess\n",
    "                             .join(_data, [variable], 'left')\n",
    "                            )\n",
    "    \n",
    "    return prm_ingresos_iess\n",
    "\n",
    "def get_lag(df: DataFrame, \n",
    "            window_partition_by: List, \n",
    "            order_by: str, \n",
    "            value: float,\n",
    "            lag: int = 1,\n",
    "            last_register: int = 1\n",
    "           ) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create dataframe partitioned by columns and orderby fecha\n",
    "    Args:\n",
    "        df:\n",
    "        window_partition_by:\n",
    "        order_by:\n",
    "        last_register:\n",
    "\n",
    "    Returns:\n",
    "        DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    w = (Window\n",
    "         .partitionBy([F.col(x) for x in window_partition_by])\n",
    "         .orderBy(order_by)\n",
    "        )\n",
    "\n",
    "    df = (df\n",
    "          .withColumn(f'lag_{lag}_{value}', \n",
    "                      F.lag(value,lag)\n",
    "                      .over(w)\n",
    "                     )\n",
    "          .withColumn(f'lag_{lag}_{value}', F.col(f'lag_{lag}_{value}').cast(\"float\"))\n",
    "         )\n",
    "    \n",
    "    df = (df\n",
    "          .withColumn(f'dif_lag_{lag}_{value}', F.col(value) - F.col(f'lag_{lag}_{value}'))\n",
    "          .withColumn(f'dif_lag_{lag}_{value}', F.col(f'dif_lag_{lag}_{value}').cast(\"float\"))\n",
    "         )\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "class TimeSeriesKFold:\n",
    "    def __init__(self, end_init_train: str, n_folds: int):\n",
    "        self.end_init_train = end_init_train\n",
    "        self.n_splits = n_folds\n",
    "\n",
    "    def split(self, X, y, groups=None):\n",
    "        for split in range(0, self.n_splits):\n",
    "            \n",
    "            dt_end_train = dt.strptime(self.end_init_train, '%Y%m').date()\n",
    "            dt_end_train = dt_end_train + relativedelta(months=(split + 1))\n",
    "            \n",
    "            end_train = int(dt_end_train.strftime('%Y%m'))\n",
    "            \n",
    "            X['yyyymm_snapshot_ind'] =  pd.to_datetime(X['yyyymm_snapshot']).dt.strftime('%Y%m').astype(int)\n",
    "            \n",
    "            \n",
    "            master_train = X[X['yyyymm_snapshot_ind'] <= (end_train)].copy()\n",
    "            \n",
    "            train_index = master_train.index\n",
    "            \n",
    "            master_test = X[X['yyyymm_snapshot_ind'] > (end_train)].copy()\n",
    "            \n",
    "            test_index = master_test.index\n",
    "            \n",
    "            yield train_index, test_index\n",
    "\n",
    "    def get_n_splits(self, X, y, groups=None):\n",
    "        return self.n_splits\n",
    "    \n",
    "    def get_n_folds(self, X, y, groups=None):\n",
    "        return self.end_init_train\n",
    "    \n",
    "def get_time_series_fold(X, y, end_init_train, n_folds):\n",
    "    \n",
    "    tsf=TimeSeriesKFold(end_init_train=end_init_train, n_folds=n_folds)\n",
    "    \n",
    "    split = tsf.split(X, y)\n",
    "    \n",
    "    return split\n",
    "\n",
    "def regex_dict(test_dict):\n",
    "    \n",
    "    new_dict = {}\n",
    "    for key, value in test_dict.items():\n",
    "        new_key = re.sub('^.*__.*__', '', key)\n",
    "        new_dict[new_key] = value\n",
    "    \n",
    "    return new_dict\n",
    "\n",
    "\n",
    "# Funcion from https://howtolearnmachinelearning.com/code-snippets/lift-curve-code-snippet/\n",
    "# Function that plots a Lift Curve using the real label values of a dataset and the probability predictions of a Machine Learning Algorithm/model\n",
    "# @Params:\n",
    "# y_val: real labels of the data\n",
    "# y_pred: probability predictions for such data\n",
    "# step: how big we want the steps in the percentiles to be\n",
    "\n",
    "def plot_lift_curve(y_val, y_pred, step=0.01):\n",
    "    \n",
    "    #Define an auxiliar dataframe to plot the curve\n",
    "    aux_lift = pd.DataFrame()\n",
    "    #Create a real and predicted column for our new DataFrame and assign values\n",
    "    aux_lift['real'] = y_val\n",
    "    aux_lift['predicted'] = y_pred\n",
    "    #Order the values for the predicted probability column:\n",
    "    aux_lift.sort_values('predicted',ascending=False,inplace=True)\n",
    "    \n",
    "    #Create the values that will go into the X axis of our plot\n",
    "    x_val = np.arange(step,1+step,step)\n",
    "    #Calculate the ratio of ones in our data\n",
    "    ratio_ones = aux_lift['real'].sum() / len(aux_lift)\n",
    "    #Create an empty vector with the values that will go on the Y axis our our plot\n",
    "    y_v = []\n",
    "    \n",
    "    #Calculate for each x value its correspondent y value\n",
    "    for x in x_val:\n",
    "        num_data = int(np.ceil(x*len(aux_lift))) #The ceil function returns the closest integer bigger than our number \n",
    "        data_here = aux_lift.iloc[:num_data,:]   # ie. np.ceil(1.4) = 2\n",
    "        ratio_ones_here = data_here['real'].sum()/len(data_here)\n",
    "        y_v.append(ratio_ones_here / ratio_ones)     \n",
    "           \n",
    "   #Plot the figure\n",
    "    fig, axis = plt.subplots()\n",
    "    fig.figsize = (40,40)\n",
    "    axis.plot(x_val, y_v, 'g-', linewidth = 3, markersize = 5)\n",
    "    axis.plot(x_val, np.ones(len(x_val)), 'k-')\n",
    "    axis.set_xlabel('Proportion of sample')\n",
    "    axis.set_ylabel('Lift')\n",
    "    plt.title('Lift Curve')\n",
    "    plt.show()\n",
    "    \n",
    "    return x_val, y_v\n",
    "\n",
    "def _get_list_of_dates(start, x_past, periods):\n",
    "    start = (dt.strptime(start, \"%Y-%m-%d\") + relativedelta(day=31)).strftime('%Y-%m-%d')\n",
    "\n",
    "    list_dates = [start]\n",
    "    \n",
    "    if periods > 1:\n",
    "        for i in range(periods -1):\n",
    "            a_date = dt.strptime(list_dates[-1], \"%Y-%m-%d\")\n",
    "            new_date = (a_date - relativedelta(months=x_past)).strftime('%Y-%m-%d')\n",
    "            new_date = (dt.strptime(new_date, \"%Y-%m-%d\") + relativedelta(day=31)).strftime('%Y-%m-%d')\n",
    "            list_dates.append(new_date)\n",
    "        \n",
    "    return list_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e7814",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06559777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### credit_scoring ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efbd3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def variable_grupos(template, data):\n",
    "    \"\"\"\n",
    "    Realiza la categorización de variables a los datos de acuerdo a un template.\n",
    "    Template: \n",
    "    Data: DataFrame\n",
    "    Output: dataframe\n",
    "    \"\"\"\n",
    "    template_n = template[template.TipoVariable==\"Numerica\"]\n",
    "    for variable in template_n.Variable.unique():\n",
    "        data[variable+\"_V\"] = data[variable]\n",
    "        \n",
    "        try:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(template[(template.Variable==variable)&(template.Nulo==\"SI\")][\"Dummy\"].iloc[0])\n",
    "            print(variable)\n",
    "        except:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(\"_null\")\n",
    "            \n",
    "#        if len(data[data[variable+\"_V\"]==\"_null\"])<0.1*len(data):\n",
    "#            data[variable+\"_V\"] = np.where(data[variable+\"_V\"]==\"_null\",\"_0\",data[variable+\"_V\"])\n",
    "\n",
    "        for i in range(len(template[template.Variable == variable] )):\n",
    "            data[variable+\"_V\"] = np.where((data[variable]>template[template.Variable == variable].Min.iloc[i])&(data[variable]<= template[template.Variable == variable].Max.iloc[i]),template[template.Variable == variable].Dummy.iloc[i],data[variable+\"_V\"])\n",
    "\n",
    "    template_c = template[template.TipoVariable==\"Categorica\"]\n",
    "    for variable in template_c.Variable.unique():\n",
    "        data[variable+\"_V\"] = data[variable]\n",
    "        try:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(template[(template.Variable==variable)&(template.Nulo==\"SI\")][\"Dummy\"].iloc[0])\n",
    "            print(variable)\n",
    "        except:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(\"_null\")\n",
    "\n",
    "        y = []    \n",
    "        for i in range(len(template[template.Variable == variable] )):\n",
    "            try:\n",
    "                x = template[template.Variable==variable][\"Lista_Categoricas\"].iloc[i].split(\"; \")\n",
    "                y = y+x\n",
    "        \n",
    "                data[variable+\"_V\"] = np.where(np.isin(data[variable],x),template[template.Variable == variable].Dummy.iloc[i],data[variable+\"_V\"])\n",
    "            except:\n",
    "                pass\n",
    "        data[variable+\"_V\"] = np.where((np.isin(data[variable],y)==False)&(data[variable].isnull()==False),template[(template.Variable==variable)&(template.Otros == \"SI\")][\"Dummy\"],data[variable+\"_V\"]) \n",
    "    return data\n",
    "    \n",
    "    \n",
    "def get_coef(model):\n",
    "    \"\"\"\n",
    "    Retorna un DataFrame con coeficientes de la RL\n",
    "    Input: modelo\n",
    "    Output: DataFrame \n",
    "    \"\"\"\n",
    "    variable = pd.DataFrame(model.params).index\n",
    "    coeficiente = pd.DataFrame(model.params).values[:,0]\n",
    "    model_coef = pd.DataFrame({\"Variable_V\": variable, \"Logistica\": coeficiente})\n",
    "    return model_coef\n",
    "\n",
    "def modelo_score(score_template):\n",
    "    \"\"\"\n",
    "    Transforma coeficientes de la RL a score sobre 1000\n",
    "    Input: DataFrame con valores de los coeficientes\n",
    "    Output: DataFrame con coeficientes y score\n",
    "    \"\"\"\n",
    "    min_values = []\n",
    "    for i in score_template.Variable.unique():\n",
    "        min_values.append(np.min(score_template[score_template.Variable == i].Logistica))\n",
    "        \n",
    "    min_logit = sum(min_values)  \n",
    "    \n",
    "    score_template[\"Score\"] = score_template.Logistica.apply(lambda x: np.round(999*x/(min_logit), 0))\n",
    "    \n",
    "    return score_template\n",
    "\n",
    "\n",
    "def score_report(template, df_coef):\n",
    "    \"\"\"\n",
    "    Devuelve el DataFrame de las transformaciones con coeficientes y score\n",
    "    Input: Datafrmae transformaciones y DataFrame con score y coeficientes\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    template[\"Variable_V\"] = template[\"Variable\"] + \"_V_\" + template[\"Dummy\"]\n",
    "    template = pd.merge(template, df_coef, on = \"Variable_V\", how = \"left\")\n",
    "    template = template.drop([\"Variable_V\"], axis=1)\n",
    "    template[\"Logistica\"] = template.Logistica.fillna(0)\n",
    "    template = modelo_score(template)\n",
    "    return template\n",
    "\n",
    "\n",
    "def correr_score(template, data):\n",
    "    \"\"\"\n",
    "    Devuelve un DataFrame con el score del cliente\n",
    "    Input: DataFrame con categorización de variables, score y coeficientes y un DataFrame\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    template_n = template[template.TipoVariable==\"Numerica\"]\n",
    "    for variable in template_n.Variable.unique():\n",
    "        data[variable+\"_V\"] = data[variable]\n",
    "        \n",
    "        try:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(template[(template.Variable==variable)&(template.Nulo==\"SI\")][\"Score\"].iloc[0])\n",
    "            # print(variable)\n",
    "        except:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(\"_null\")\n",
    "            \n",
    "#        if len(data[data[variable+\"_V\"]==\"_null\"])<0.1*len(data):\n",
    "#            data[variable+\"_V\"] = np.where(data[variable+\"_V\"]==\"_null\",\"_0\",data[variable+\"_V\"])\n",
    "\n",
    "        for i in range(len(template[template.Variable == variable] )):\n",
    "            data[variable+\"_V\"] = np.where((data[variable]>template[template.Variable == variable].Min.iloc[i])&(data[variable]<= template[template.Variable == variable].Max.iloc[i]),template[template.Variable == variable].Score.iloc[i],data[variable+\"_V\"])\n",
    "\n",
    "    template_c = template[template.TipoVariable==\"Categorica\"]\n",
    "    for variable in template_c.Variable.unique():\n",
    "        data[variable+\"_V\"] = data[variable]\n",
    "        try:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(template[(template.Variable==variable)&(template.Nulo==\"SI\")][\"Score\"].iloc[0])\n",
    "            # print(variable)\n",
    "        except:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(\"_null\")\n",
    "\n",
    "        y = []    \n",
    "        for i in range(len(template[template.Variable == variable] )):\n",
    "            try:\n",
    "                x = template[template.Variable==variable][\"Lista_Categoricas\"].iloc[i].split(\"; \")\n",
    "                y = y+x\n",
    "        \n",
    "                data[variable+\"_V\"] = np.where(np.isin(data[variable],x),template[template.Variable == variable].Score.iloc[i],data[variable+\"_V\"])\n",
    "            except:\n",
    "                pass\n",
    "        data[variable+\"_V\"] = np.where((np.isin(data[variable],y)==False)&(data[variable].isnull()==False),template[(template.Variable==variable)&(template.Otros == \"SI\")][\"Score\"],data[variable+\"_V\"]) \n",
    "    data[\"score_riesgo_AA\"] = data[template.Variable.unique() + \"_V\"].sum(axis=1)\n",
    "    return data\n",
    "\n",
    "def tabla_resumen(data, name_order, target, division=10):\n",
    "    \"\"\"\n",
    "    Devuelve un DataFrame con el resumen del modelo\n",
    "    Input: data, name_order(nombre de la columna de score o probabilidad), target: nombre de la Y,\n",
    "           division (número de buckets)\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=name_order, ascending=True).reset_index(drop=True)\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], division)\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    df_resumen = pd.DataFrame()\n",
    "    df_resumen[\"min_score\"] = grouped.min()[name_order]\n",
    "    df_resumen[\"max_score\"] = grouped.max()[name_order]\n",
    "    df_resumen[\"total\"] = grouped.count()[target]\n",
    "    df_resumen[\"events\"] = grouped.sum()[target]\n",
    "    df_resumen[\"non_events\"] = grouped.count()[target] - grouped.sum()[target]\n",
    "\n",
    "    df_resumen[\"per_events\"] = df_resumen[\"events\"]/df_resumen[\"total\"]\n",
    "    df_resumen[\"per_non_events\"] = df_resumen[\"non_events\"]/df_resumen[\"total\"]\n",
    "    df_resumen[\"odds\"] = df_resumen[\"per_events\"]/df_resumen[\"per_non_events\"]\n",
    "    df_resumen[\"event_rate\"] = df_resumen[\"events\"]/df_resumen[\"events\"].sum()\n",
    "    df_resumen[\"cum_event_rate\"] = df_resumen[\"event_rate\"].cumsum()\n",
    "    df_resumen[\"non_event_rate\"] = df_resumen[\"non_events\"]/df_resumen[\"non_events\"].sum()\n",
    "    df_resumen[\"cum_non_event_rate\"] = df_resumen[\"non_event_rate\"].cumsum()\n",
    "    df_resumen[\"ks\"] = np.round((df_resumen[\"cum_event_rate\"] - df_resumen[\"cum_non_event_rate\"]), 3) * 100\n",
    "    \n",
    "    return df_resumen\n",
    "\n",
    "def tabla_resumen_prob(data, name_order, target, prob, division=10):\n",
    "    \"\"\"\n",
    "    Devuelve un DataFrame con el resumen del modelo agregada la probabilidad\n",
    "    Input: data, name_order(nombre de la columna de score o probabilidad), target: nombre de la Y,\n",
    "           division (número de buckets)\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=name_order, ascending=True).reset_index(drop=True)\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], division)\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    df_resumen = pd.DataFrame()\n",
    "    df_resumen[\"min_score\"] = grouped.min()[name_order]\n",
    "    df_resumen[\"max_score\"] = grouped.max()[name_order]\n",
    "    df_resumen[\"min_prob\"] = grouped.min()[prob]\n",
    "    df_resumen[\"max_prob\"] = grouped.max()[prob]\n",
    "    df_resumen[\"total\"] = grouped.count()[target]\n",
    "    df_resumen[\"events\"] = grouped.sum()[target]\n",
    "    df_resumen[\"non_events\"] = grouped.count()[target] - grouped.sum()[target]\n",
    "\n",
    "    df_resumen[\"per_events\"] = df_resumen[\"events\"]/df_resumen[\"total\"]\n",
    "    df_resumen[\"per_non_events\"] = df_resumen[\"non_events\"]/df_resumen[\"total\"]\n",
    "    df_resumen[\"odds\"] = df_resumen[\"per_events\"]/df_resumen[\"per_non_events\"]\n",
    "    df_resumen[\"event_rate\"] = df_resumen[\"events\"]/df_resumen[\"events\"].sum()\n",
    "    df_resumen[\"cum_event_rate\"] = df_resumen[\"event_rate\"].cumsum()\n",
    "    df_resumen[\"non_event_rate\"] = df_resumen[\"non_events\"]/df_resumen[\"non_events\"].sum()\n",
    "    df_resumen[\"cum_non_event_rate\"] = df_resumen[\"non_event_rate\"].cumsum()\n",
    "    df_resumen[\"ks\"] = np.round((df_resumen[\"cum_event_rate\"] - df_resumen[\"cum_non_event_rate\"]), 3) * 100\n",
    "    df_resumen = df_resumen.sort_values(by=\"max_score\", ascending=False)\n",
    "    \n",
    "    return df_resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cef7098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### data_processing ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46060c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cut_percentile(data,variable, percentile, tail):\n",
    "    \"\"\"\n",
    "    tail can take 3 values: right, left, and both\n",
    "    percentile must be a list [percentile] if tail = right or tail=left, and  [low percentile, high percentile] if tail=both\n",
    "    percentile can take values from 1 to 99\n",
    "    \"\"\"\n",
    "    \n",
    "    if tail == \"right\":\n",
    "        percentile_variable = np.percentile(data[variable].dropna(),percentile[0])\n",
    "        data_return = data[data[variable]<=percentile_variable]\n",
    "        \n",
    "    if tail == \"left\":\n",
    "        percentile_variable = np.percentile(data[variable].dropna(),percentile[0])\n",
    "        data_return = data[data[variable]>=percentile_variable]\n",
    "        \n",
    "    if tail == \"both\":\n",
    "        low_percentile = np.percentile(data[variable].dropna(),percentile[0])\n",
    "        high_percentile = np.percentile(data[variable].dropna(),percentile[1])\n",
    "        data_return = data[(data[variable]>=low_percentile)&(data[variable]<=high_percentile)]\n",
    "        \n",
    "    return data_return\n",
    "    \n",
    "    \n",
    "def group_category(data, variable, dict_variable):\n",
    "    \n",
    "    for i in dict_variable:\n",
    "        data[variable] = np.where(np.isin(data[variable], dict_variable[i]), i, data[variable])\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def rango_valores(df, intervalo, variable):\n",
    "    \n",
    "    df = df[(df[variable]>intervalo[0])&(df[variable]<=intervalo[1])]\n",
    "    return df\n",
    "\n",
    "def category_str(data, categorical_vr):\n",
    "    \"\"\"\n",
    "    Retorna el DataFrame transformando las variables categóricas a strings\n",
    "    Input: DataFrame\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    for i in categorical_vr:\n",
    "        data[i] = data[i].apply(lambda x: str(x))\n",
    "    return data\n",
    "\n",
    "def small_group(data, variable, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evalua aquellas categorías de una variable que represeten menos del \"n\" porciento\n",
    "    variable: nombre de la variable (string)\n",
    "    n debe ser medido en %: es decir 0-100%\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(data[variable].value_counts(normalize=True)*100)\n",
    "\n",
    "    df2 = pd.DataFrame({variable:df.index, \"porcentaje\":df[variable]})\n",
    "\n",
    "    df2 = df2.reset_index().drop(columns = [\"index\"])\n",
    "    \n",
    "    return df2[df2[\"porcentaje\"]<=n][variable].unique().tolist()\n",
    "\n",
    "def replace_low_freq(data, variable, lista_reemplazo, reemplazo=\"OTRO\"):\n",
    "    \"\"\"\n",
    "    Reemplaza los valores de un columna de un DF de strings\n",
    "    data: DataFrame\n",
    "    variable: nomnbre de variable (string)\n",
    "    lista_remplazo: lista de string (list)\n",
    "    reemplazo: string de reemplazo\n",
    "    return Dataframe remplazada la variable\n",
    "    \"\"\"\n",
    "    data[variable] = np.where(np.isin(data[variable],\n",
    "                                      lista_reemplazo),\n",
    "                                reemplazo, data[variable])\n",
    "    return data\n",
    "\n",
    "def reg_qant(target, var):\n",
    "    \"\"\"\n",
    "    Transforma una lista de variables en un string para entrar en la regresión cuantílica de statsmodel\n",
    "    \"\"\"\n",
    "    \n",
    "    string = target + \" ~ \"\n",
    "    for i in var:\n",
    "        string = string + i + \" + \"\n",
    "    return string[:-3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d423a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### eda ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3286a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "from optbinning import OptimalBinning\n",
    "\n",
    "def null_function(data):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame con el número de valores nulos y\n",
    "    su respectivo porcentaje.\n",
    "    Input: DataFrame\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    len_list = []\n",
    "    null_list = []\n",
    "    null_per_list = []\n",
    "    for i in data.columns:\n",
    "        len_v = len(data)\n",
    "        len_list.append(len_v)\n",
    "        null_v = data[i].isnull().sum()\n",
    "        null_list.append(null_v)\n",
    "        null_per_v = null_v/len_v\n",
    "        null_per_list.append(null_per_v)\n",
    "\n",
    "    null_df = pd.DataFrame({\"variable\": data.columns,\n",
    "                            \"num_datos\": len_list,\n",
    "                            \"num_null\": null_list,\n",
    "                            \"per_null\": null_per_list})\n",
    "    return null_df\n",
    "\n",
    "def categorico_numerico(data, thr_numerico=100, thr_categorico=24, remove_list = []):\n",
    "    \"\"\"\n",
    "    Crea un DataFrame dividiendo las variables en\n",
    "    numéricas, categóricas y revisión\n",
    "    Input: datos, umbral numerico y umbral categorico\n",
    "    output: dataFrame\n",
    "    \"\"\"\n",
    "    data = data.drop(remove_list, axis=1)\n",
    "    df_nc = pd.DataFrame()\n",
    "    df_nc[\"variable\"] = data.columns\n",
    "    df_nc[\"unico\"] = [len(data[i].unique()) for i in data.columns]\n",
    "    df_nc[\"tipo\"] = np.where(df_nc[\"unico\"] > thr_numerico, \"NUMERICA\",\n",
    "                             np.where(df_nc[\"unico\"] > thr_categorico,\n",
    "                                      \"REVISAR\",\n",
    "                                      \"CATEGORICA\"))\n",
    "    return df_nc\n",
    "\n",
    "def cambio_revision(df_nc, num_list, cat_list):\n",
    "    \"\"\"\n",
    "    Cambiar las vairables en listas numericas y listas categoricas\n",
    "    por el tipo correspondiente\n",
    "    input: DataFrame que devuelve la funcion cateogirco_numerico\n",
    "    output: DataFrame\n",
    "    \"\"\"\n",
    "    df_nc[\"tipo_final\"] = np.where(np.isin(df_nc[\"variable\"],\n",
    "                                           num_list), \"NUMERICA\",\n",
    "                                   np.where(np.isin(df_nc[\"variable\"],\n",
    "                                                    cat_list), \"CATEGORICA\",\n",
    "                                            df_nc[\"tipo\"]))\n",
    "    return df_nc\n",
    "\n",
    "def category_eda(data, categorical_vr):\n",
    "    \"\"\"\n",
    "    Crea un Dataframe con estadísticso descriptivos\n",
    "    para las variables categóricas de un conjunto de datos.\n",
    "    Los datos entregados son: número de categorías, moda, etc.\n",
    "    Input: Dataframe y lista\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    category_list = []\n",
    "    num_category_list = []\n",
    "    mode_list = []\n",
    "    mode_count_list = []\n",
    "    len_list = []\n",
    "    count_list = []\n",
    "    mode_per_list = []\n",
    "    mode_per_sin_null_list = []\n",
    "    for i in categorical_vr:\n",
    "        category_v = data[i].unique()\n",
    "        category_list.append(category_v)\n",
    "        num_category_v = len(category_v)\n",
    "        num_category_list.append(num_category_v)\n",
    "        mode_v = data[i].mode()[0]\n",
    "        mode_list.append(mode_v)\n",
    "        mode_count_v = len(data[i][data[i] == mode_v])\n",
    "        mode_count_list.append(mode_count_v)\n",
    "        len_v = len(data)\n",
    "        len_list.append(len_v)\n",
    "        count_v = data[i].count()\n",
    "        count_list.append(count_v)\n",
    "        mode_per_v = mode_count_v/len_v\n",
    "        mode_per_list.append(mode_per_v)\n",
    "        mode_per_sin_null_v = mode_count_v/count_v\n",
    "        mode_per_sin_null_list.append(mode_per_sin_null_v)\n",
    "\n",
    "    category_df = pd.DataFrame({\"variable\": categorical_vr,\n",
    "                                \"num_category\": num_category_list,\n",
    "                                \"mode\": mode_list,\n",
    "                                \"mode_count\": mode_count_list,\n",
    "                                \"num_datos\": len_list,\n",
    "                                \"count_datos\": count_list,\n",
    "                                \"mode_per_total\": mode_per_list,\n",
    "                                \"mode_per_sin_null\": mode_per_sin_null_list,\n",
    "                                \"category\": category_list})\n",
    "    return category_df\n",
    "\n",
    "def numeric_eda(data, numerical_list):\n",
    "    \"\"\"\n",
    "    Entrega un resumen con las estadísticas descriptivas\n",
    "    de un conjunto de datos numéricos\n",
    "    Estadísticos entregados: media,\n",
    "    percentil 1, 5, 25, 50, 75, 95 y 99, max, min, etc.\n",
    "    Input: Dataframe\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    len_list = []\n",
    "    null_list = []\n",
    "    null_per_list = []\n",
    "    mean_list = []\n",
    "    std_list = []\n",
    "    min_list = []\n",
    "    per_1_list = []\n",
    "    per_5_list = []\n",
    "    per_25_list = []\n",
    "    median_list = []\n",
    "    per_75_list = []\n",
    "    per_95_list = []\n",
    "    per_99_list = []\n",
    "    max_list = []\n",
    "    \n",
    "    data = data[numerical_list]\n",
    "\n",
    "    for i in data.columns:\n",
    "        len_v = len(data)\n",
    "        len_list.append(len_v)\n",
    "        null_v = data[i].isnull().sum()\n",
    "        null_list.append(null_v)\n",
    "        null_per_v = null_v/len_v\n",
    "        null_per_list.append(null_per_v)\n",
    "        mean_v = np.mean(data[i])\n",
    "        mean_list.append(mean_v)\n",
    "        std_v = np.std(data[i])\n",
    "        std_list.append(std_v)\n",
    "        min_v = np.min(data[i])\n",
    "        min_list.append(min_v)\n",
    "        per_1_v = np.percentile(data[i].dropna(), 1)\n",
    "        per_1_list.append(per_1_v)\n",
    "        per_5_v = np.percentile(data[i].dropna(), 5)\n",
    "        per_5_list.append(per_5_v)\n",
    "        per_25_v = np.percentile(data[i].dropna(), 25)\n",
    "        per_25_list.append(per_25_v)\n",
    "        median_v = np.median(data[i].dropna())\n",
    "        median_list.append(median_v)\n",
    "        per_75_v = np.percentile(data[i].dropna(), 75)\n",
    "        per_75_list.append(per_75_v)\n",
    "        per_95_v = np.percentile(data[i].dropna(), 95)\n",
    "        per_95_list.append(per_95_v)\n",
    "        per_99_v = np.percentile(data[i].dropna(), 99)\n",
    "        per_99_list.append(per_99_v)\n",
    "        max_v = np.max(data[i])\n",
    "        max_list.append(max_v)\n",
    "\n",
    "    numeric_df = pd.DataFrame({\"variable\": data.columns,\n",
    "                               \"num_datos\": len_list,\n",
    "                               \"num_null\": null_list,\n",
    "                               \"per_null\": null_per_list,\n",
    "                               \"mean\": mean_list,\n",
    "                               \"std\": std_list,\n",
    "                               \"min\": min_list,\n",
    "                               \"per_1\": per_1_list,\n",
    "                               \"per_5\": per_5_list,\n",
    "                               \"per_25\": per_25_list,\n",
    "                               \"median\": median_list,\n",
    "                               \"per_75\": per_75_list,\n",
    "                               \"per_95\": per_95_list,\n",
    "                               \"per_99\": per_99_list,\n",
    "                               \"max\": max_list\n",
    "                               })\n",
    "\n",
    "    return numeric_df\n",
    "\n",
    "def woe_categoric(data, y, var):\n",
    "    \"\"\"\n",
    "    Calcula el cuadro del Weight of Evidence de una variable categórica\n",
    "    Input: DataFrame, Nombre variable dependiente, nombre de variable\n",
    "    independiente\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    data = data[[y, var]]\n",
    "    data = data.dropna()\n",
    "    df_c = pd.DataFrame()\n",
    "    df_c[\"variable\"] = data[var].unique()\n",
    "    df_c[\"non_event\"] = df_c.variable\\\n",
    "        .apply(lambda x: data[(data[var] == x) &\n",
    "                              (data[y] == 0)][y].count())\n",
    "    df_c[\"event\"] = df_c.variable\\\n",
    "        .apply(lambda x: data[(data[var] == x) &\n",
    "                              (data[y] == 1)][y].count())\n",
    "\n",
    "    df_c[\"per_non_event\"] = df_c.non_event/df_c[\"non_event\"].sum()\n",
    "    df_c[\"per_event\"] = df_c.event/df_c[\"event\"].sum()\n",
    "    df_c[\"woe\"] = np.log(df_c.per_event/df_c.per_non_event)\n",
    "    df_c[\"woe\"] = np.where(np.abs(df_c.woe)==np.inf, 0, df_c.woe)\n",
    "\n",
    "    df_c[\"per_event_per_non_event\"] = df_c.per_event - df_c.per_non_event\n",
    "    df_c[\"iv\"] = df_c.woe * (df_c.per_event - df_c.per_non_event)\n",
    "    return df_c\n",
    "\n",
    "def iv_categoric(data, y, var):\n",
    "    \"\"\"\n",
    "    Calcula el information value de una variable categórica\n",
    "    Input: DataFrame, nombre variable dependiente(string),\n",
    "    nombre variable independiente(string)\n",
    "    Output: float\n",
    "    \"\"\"\n",
    "    iv = woe_categoric(data, y, var).iv.sum()\n",
    "    return iv\n",
    "\n",
    "def woe_numeric(data, y, var, bins=5):\n",
    "    \"\"\"\n",
    "    Calcula el cuadro del Weight of Evidence de una variable numérica\n",
    "    Input: DataFrame, Nombre variable dependiente,\n",
    "    nombre de variable independiente\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    data = data[[y, var]]\n",
    "    data = data.dropna()\n",
    "    data = data.sort_values(by=var, ascending=True).reset_index(drop=True)\n",
    "    data[\"index_\"] = data.index  # create a column of index\n",
    "    data['decile'] = pd.qcut(data[\"index_\"], bins)  # Create deciles\n",
    "\n",
    "    data[\"event\"] = data[y]\n",
    "    data[\"non_event\"] = 1 - data[y]\n",
    "\n",
    "    grouped = data.groupby('decile', as_index=False)\n",
    "\n",
    "    df_num = pd.DataFrame()\n",
    "\n",
    "    df_num['min'] = grouped.min()[var]\n",
    "    df_num['max'] = grouped.max()[var]\n",
    "    df_num[\"event\"] = grouped.sum()['event']\n",
    "    df_num[\"non_event\"] = grouped.sum()['non_event']\n",
    "    df_num[\"per_event\"] = df_num[\"event\"]/df_num[\"event\"].sum()\n",
    "    df_num[\"per_non_event\"] = df_num[\"non_event\"]/df_num[\"non_event\"].sum()\n",
    "\n",
    "    df_num[\"woe\"] = np.log(df_num.per_event/df_num.per_non_event)\n",
    "    df_num[\"per_event_per_non_event\"] = df_num.per_event - df_num.per_non_event\n",
    "    df_num[\"iv\"] = df_num.woe * (df_num.per_event - df_num.per_non_event)\n",
    "\n",
    "    return df_num\n",
    "\n",
    "def monotonic(x):\n",
    "    \"\"\"\n",
    "    Verifica que una lista es monotócitamente creciente o decreciente\n",
    "    Input: lista de valores numéricos\n",
    "    Output: Booleano\n",
    "    \"\"\"\n",
    "    dx = np.diff(x)\n",
    "    return np.all(dx <= 0) or np.all(dx >= 0)\n",
    "\n",
    "def iv_numeric(data, y, var):\n",
    "    \"\"\"\n",
    "    Calcula el information value de una variable numérica\n",
    "    Input: DataFrame, nombre variable dependiente(string),\n",
    "    nombre variable independiente(string)\n",
    "    Output: float\n",
    "    \"\"\"\n",
    "    df_iv = pd.DataFrame()\n",
    "\n",
    "    df_iv[\"monotonic\"] = [monotonic\n",
    "                          (woe_numeric(data,\n",
    "                                       y, var,\n",
    "                                       j).woe) for j in range(1, 11)]\n",
    "\n",
    "    max_woe = len(df_iv.query(\"monotonic == True\"))\n",
    "    iv = woe_numeric(data, y, var, bins=max_woe).iv.sum()\n",
    "    return iv\n",
    "\n",
    "def iv_df(data, y, categoric_list, numeric_list):\n",
    "    \"\"\"\n",
    "    Retorna un DataFrame ordenado del IV de cada variable de un DataFrame\n",
    "    Input: DataFrame, variable dependiente,\n",
    "    lista de variables independientes categóricas, numéricas\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    df_iv = pd.DataFrame()\n",
    "    iv_c_list = [iv_categoric(data, y, i) for i in categoric_list]\n",
    "    iv_n_list = [iv_numeric(data, y, i) for i in numeric_list]\n",
    "    df_iv[\"variable\"] = categoric_list + numeric_list\n",
    "    df_iv[\"iv\"] = iv_c_list + iv_n_list\n",
    "    df_iv = df_iv.sort_values(by=\"iv\", ascending=False)\n",
    "    return df_iv\n",
    "\n",
    "def small_group(data, variable, n):\n",
    "    \n",
    "    \"\"\"\n",
    "    Evalua aquellas categorías de una variable que represeten menos del \"n\" porciento\n",
    "    \n",
    "    n debe ser medio en %: es decir 0-100%\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame(data[variable].value_counts(normalize=True)*100)\n",
    "\n",
    "    df2 = pd.DataFrame({variable:df.index, \"porcentaje\":df[variable]})\n",
    "\n",
    "    df2 = df2.reset_index().drop(columns = [\"index\"])\n",
    "    \n",
    "    return df2[df2[\"porcentaje\"]<=n][variable].unique().tolist()\n",
    "\n",
    "\n",
    "def cramers_V(var1, var2) :\n",
    "    \"\"\"\n",
    "    Calcula el estadístico de cramer para dos variables\n",
    "    Input: dos listas, columns\n",
    "    Output: número\n",
    "    \"\"\"\n",
    "    crosstab =np.array(pd.crosstab(var1, var2, rownames=None, colnames=None))\n",
    "    stat = chi2_contingency(crosstab)[0]\n",
    "    obs = np.sum(crosstab) \n",
    "    mini = min(crosstab.shape)-1 \n",
    "    return (stat/(obs*mini))\n",
    "\n",
    "\n",
    "def cramers_v_matrix(data, variable_list):\n",
    "    \"\"\"\n",
    "    Calcula la matriz de Cramer para un conjunto de datos\n",
    "    Input: DataFrame, lista de variables\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    rows= []\n",
    "    data = data[variable_list]\n",
    "    for var1 in data:\n",
    "\n",
    "        col = []\n",
    "        for var2 in data:\n",
    "            cramers =cramers_V(data[var1], data[var2]) # Cramer's V test\n",
    "            col.append(round(cramers,2)) # Keeping of the rounded value of the Cramer's V  \n",
    "        rows.append(col)\n",
    "\n",
    "    cramers_results = np.array(rows)\n",
    "    df_cramer = pd.DataFrame(cramers_results, columns = data.columns, index =data.columns)\n",
    "    return df_cramer\n",
    "\n",
    "def pearson_matrix(data, numerical_list):\n",
    "    \"\"\"\n",
    "    Calcula la matriz de correlación para variables numéricas\n",
    "    Input: DF, lista de variables\n",
    "    Output: DF\n",
    "    \"\"\"\n",
    "    data = data[numerical_list]\n",
    "    df_pearson = data.corr(method = \"pearson\")\n",
    "    return df_pearson\n",
    "\n",
    "def list_correlation(corr_matrix, threshold=0.4):\n",
    "    \"\"\"\n",
    "    Una vez obtenida la matriz de correlacion, se encuentran\n",
    "    las variables corrrelacionadas con cada varaible\n",
    "    Input: DF matriz de correlación y umbral:número\n",
    "    Output: DF\n",
    "    \"\"\"\n",
    "    corr_list = []\n",
    "    for i in corr_matrix.columns:\n",
    "        corr_var = list(corr_matrix[corr_matrix[i] > threshold ].index)\n",
    "        try:\n",
    "            corr_var.remove(i)\n",
    "        except:\n",
    "            pass\n",
    "        corr_list.append(corr_var)\n",
    "    df_corr = pd.DataFrame({\"variable\": corr_matrix.columns,\n",
    "                                 \"correlacion\": corr_list})\n",
    "    return df_corr\n",
    "\n",
    "def seleccion_categoric_iv(data, df_iv, corr_umbral=0.4, iv_umbral=0.02):\n",
    "    \"\"\"\n",
    "    Retorna una lista de las posibles variables categoricas que entran al modelo\n",
    "    Input: DataFrame, dataframe de iv y umbral\n",
    "    Output:lista\n",
    "    \"\"\"\n",
    "    df_iv = df_iv.sort_values(by=\"iv\", ascending=False)\n",
    "    df_iv = df_iv[df_iv.iv > iv_umbral]\n",
    "    prob_list = []\n",
    "    for i in df_iv.variable:\n",
    "        prob_list.append(i)\n",
    "        eda_df_corr = list_correlation(cramers_v_matrix(data, prob_list), threshold=corr_umbral)\n",
    "        if eda_df_corr.correlacion.sum() != []:\n",
    "            prob_list.remove(i)\n",
    "    return prob_list\n",
    "\n",
    "def seleccion_numeric_iv(data, df_iv, corr_umbral=0.6, iv_umbral=0.02):\n",
    "    \"\"\"\n",
    "    Retorna una lista de las posibles variables numéricas que entran al modelo\n",
    "    Input: DataFrame, dataframe de iv y umbral\n",
    "    Output:lista\n",
    "    \"\"\"\n",
    "    df_iv = df_iv.sort_values(by=\"iv\", ascending=False)\n",
    "    df_iv = df_iv[df_iv.iv > iv_umbral]\n",
    "    prob_list = []\n",
    "    for i in df_iv.variable:\n",
    "        prob_list.append(i)\n",
    "        eda_df_corr = list_correlation(pearson_matrix(data, prob_list), threshold=corr_umbral)\n",
    "        if eda_df_corr.correlacion.sum() != []:\n",
    "            prob_list.remove(i)\n",
    "    return prob_list\n",
    "\n",
    "def calc_vif(X):\n",
    "    \"\"\"\n",
    "    Perform VIF analysis\n",
    "    Input: DataFrame\n",
    "    Output: Dataframe\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "    return(vif)\n",
    "\n",
    "def iv_df_opt(data, cat_list, num_list, target):\n",
    "    \"\"\"\n",
    "    Calculate information value of a set of variables using optimal binning\n",
    "    Input: DataFrame, list of categorical variables, list of numerical variables\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    y = data[target]\n",
    "    iv_var_cat = []\n",
    "    iv_var_num = []\n",
    "\n",
    "    for i in cat_list:\n",
    "        x = data[i].values\n",
    "        optb = OptimalBinning(name=i, dtype=\"categorical\", solver=\"cp\", max_n_bins=8)\n",
    "        optb.fit(x, y)\n",
    "        binning_table = optb.binning_table.build()\n",
    "        iv = binning_table.IV.loc[\"Totals\"]\n",
    "        iv_var_cat.append(iv)\n",
    "\n",
    "    for i in num_list:\n",
    "        x = data[i].values\n",
    "        optb = OptimalBinning(name=i, dtype=\"numerical\", solver=\"cp\", max_n_bins=8)\n",
    "        optb.fit(x, y)\n",
    "        binning_table = optb.binning_table.build()\n",
    "        iv = binning_table.IV.loc[\"Totals\"]\n",
    "        iv_var_num.append(iv)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df[\"variable\"] = cat_list + num_list\n",
    "    df[\"iv\"] = iv_var_cat + iv_var_num\n",
    "    df = df.sort_values(by=\"iv\", ascending=False).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def binning_var(data, cat_list, num_list):\n",
    "    \"\"\"\n",
    "    Categorize variables\n",
    "    Input: DataFrame, list of categorical variables, list of numerical variables\n",
    "    Output: DataFrame\n",
    "    \"\"\"\n",
    "    data_cat = pd.DataFrame()\n",
    "    y = data.target\n",
    "    iv_var_cat = []\n",
    "    for i in cat_list:\n",
    "        x = data[i].values\n",
    "        optb = OptimalBinning(name=i, dtype=\"categorical\", solver=\"cp\", max_n_bins=8)\n",
    "        optb.fit(x, y)\n",
    "        x_transform_bins = optb.transform(x, metric=\"bins\")\n",
    "        data_cat[i] = x_transform_bins\n",
    "\n",
    "    y = data.target\n",
    "    iv_var_num = []\n",
    "    for i in num_list:\n",
    "        x = data[i].values\n",
    "        optb = OptimalBinning(name=i, dtype=\"numerical\", solver=\"cp\", max_n_bins=8)\n",
    "        optb.fit(x, y)\n",
    "        x_transform_bins = optb.transform(x, metric=\"bins\")\n",
    "        data_cat[i] = x_transform_bins\n",
    "    return data_cat\n",
    "\n",
    "def select_variable(corr_list, df_iv, iv_umbral):\n",
    "    \"\"\"\n",
    "    Perform the selection of variables considering IV and correlation between categorical data\n",
    "    Input:\n",
    "            corr_list: DataFrame, output of list_correlation function\n",
    "            df_iv: DataFrame, information value per variable\n",
    "            iv_umbral: float, minimun iv accepted\n",
    "    Output:\n",
    "            list\n",
    "    \"\"\"\n",
    "    df_iv_corr = df_iv.merge(corr_list, on=\"variable\")\n",
    "    df_iv_corr = df_iv_corr.sort_values(by=\"iv\", ascending=False)\n",
    "    df_iv_corr = df_iv_corr[df_iv_corr.iv > iv_umbral]\n",
    "\n",
    "    prob_list = []\n",
    "    rem_list = []\n",
    "    for i in df_iv_corr.variable:\n",
    "        if i not in rem_list:\n",
    "            corr_var = [i] + df_iv_corr[df_iv_corr[\"variable\"]==i].correlacion.iloc[0]\n",
    "            df_corr_var = df_iv_corr[np.isin(df_iv_corr.variable, corr_var)]\n",
    "            prob_list.append(df_corr_var.loc[df_corr_var[\"iv\"].idxmax()].variable)\n",
    "            corr_var.remove(df_corr_var.loc[df_corr_var[\"iv\"].idxmax()].variable)\n",
    "            rem_list = rem_list+corr_var\n",
    "        if i in rem_list:\n",
    "            pass\n",
    "    return prob_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c59434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### metrics ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876d94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import risk_stream.utils.credit_scoring as cs\n",
    "\n",
    "def ks_metric(data, probability, real):\n",
    "    \"\"\"\n",
    "    Return ks metric of a data set\n",
    "    Data: data frame containing probability predicted dependent variable and real dependent variable\n",
    "    Probability: name of the column of probabilities\n",
    "    Real: name of the column of the real dependent variable\n",
    "    Event = 1\n",
    "    No event =0\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=probability, ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], 10) # Create deciles\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    cumsum = 0\n",
    "    ks_list = []\n",
    "    \n",
    "    list_cumPercentage = []\n",
    "    percentage = (grouped.count()[real]-grouped.sum()[real])/((grouped.count()[real]-grouped.sum()[real]).sum()) #calculate the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum = cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "        \n",
    "    cumsum_no_event = pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event = (grouped.sum()[real]/grouped.sum()[real].sum()).cumsum() #acumulative sum of the percentages of the events\n",
    "    ks = cumsum_no_event-cumsum_event\n",
    "    \n",
    "    return max(ks)\n",
    "                     \n",
    "               \n",
    "def adjusted_prediction(y_true, probability, low_cut_probability, high_cut_probability):\n",
    "    \"\"\"\n",
    "    Return a Data Frame with the prediction of the dependent variable according to a low and high probability\n",
    "    Eliminates the row with probability bigger than the low probability and lower than the high probability\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'Probability':probability,'True Dependent Variable': y_true})\n",
    "    df['Adjusted Prediction'] = np.where(df['Probability']<=low_cut_probability,0,\n",
    "                                        np.where(df['Probability']>=high_cut_probability,1,2))\n",
    "    return df[df['Adjusted Prediction']!=2]\n",
    "\n",
    "               \n",
    "               \n",
    "def model_metrics(data):\n",
    "    \"\"\"\n",
    "    Print Accuracy, KS, Gini,and other metrics of the model\n",
    "    Data must contain columns of the Adjusted y, Probability and True y\n",
    "    \"\"\"\n",
    "    prediction = data['Adjusted Prediction'] #Take the predicted Y \n",
    "    probability = data['Probability'] #Take the probabilities f\n",
    "    y_true = data['True Dependent Variable']\n",
    "    \n",
    "    print(\"Accuracy:\",accuracy_score(y_true, prediction))\n",
    "    print('ks:',ks_metric(data,'Probability','True Dependent Variable'))\n",
    "    print(\"ROC AUC:\", roc_auc_score(y_true,probability))\n",
    "    print(\"Gini:\", (2*roc_auc_score(y_true, probability) -1))\n",
    "    tp, fn, fp, tn = confusion_matrix(y_true,prediction).ravel() #Positive=0 and Negative=1\n",
    "    print('Negative Predicted value: ',(tn/(tn+fn)) )\n",
    "    print('Positive Predicted value: ',(tp/(tp+fp)))\n",
    "    print('Sensitivity: ',(tp/(tp+fn)) )\n",
    "    print('Specificity: ',(tn/(fp+tn)) )\n",
    "    print('tp, fn, fp, tn=: ',[tp, fn, fp, tn])\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true,prediction))\n",
    "    print(classification_report(y_true,prediction))    \n",
    "               \n",
    "               \n",
    "def plot_model(df):\n",
    "    \n",
    "    #ROC AUC curve\n",
    "    ns_probs = [0 for _ in range(len(df['True Dependent Variable']))]\n",
    "    lr_probs = df['Probability']\n",
    "    ns_fpr, ns_tpr, _ = roc_curve(df['True Dependent Variable'], ns_probs)\n",
    "    lr_fpr, lr_tpr, _ = roc_curve(df['True Dependent Variable'], lr_probs)\n",
    "\n",
    "    #Odds Ratio\n",
    "\n",
    "    df_decil = df.sort_values(by=\"Probability\", ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    df_decil[\"Index\"]=df_decil.index  #create a column of index\n",
    "    df_decil['Decile'] = pd.qcut(df_decil[\"Index\"], 10) # Create deciles\n",
    "    grouped = df_decil.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    odds_train = pd.DataFrame({'% of Bad Clients': grouped.sum()['True Dependent Variable']/grouped.count()['True Dependent Variable']})\n",
    "    odds_train['Odds'] = odds_train['% of Bad Clients'].apply(lambda p: p/(1-p) if p<1 else 0.99/0.01)\n",
    "\n",
    "    cumsum=0 #used in the for loop\n",
    "    list_cumPercentage=[] #used in the for loop\n",
    "    percentage=(grouped.count()[\"True Dependent Variable\"]-grouped.sum()[\"True Dependent Variable\"])/((grouped.count()[\"True Dependent Variable\"]-grouped.sum()[\"True Dependent Variable\"]).sum()) #calculta the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum=cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "\n",
    "    cumsum_no_event=pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event=(grouped.sum()[\"True Dependent Variable\"]/grouped.sum()[\"True Dependent Variable\"].sum()).cumsum()\n",
    "\n",
    "    #GRAPHS\n",
    "    fig, axs = plt.subplots(2, 2,figsize=(12,12))\n",
    "\n",
    "    #ROC \n",
    "    axs[0, 0].plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n",
    "    axs[0, 0].plot(lr_fpr, lr_tpr, marker='.', label='Model')\n",
    "    axs[0, 0].set_xlabel('False Positive Rate')\n",
    "    axs[0, 0].set_ylabel('True Positive Rate')\n",
    "    axs[0,0].set_title('ROC')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    #Probability Histogram\n",
    "\n",
    "    axs[0,1].hist(df[df['True Dependent Variable']==0]['Probability'],color='purple',label='Non Event')\n",
    "    axs[0,1].hist(df[df['True Dependent Variable']==1]['Probability'],color='orange', label='Event')\n",
    "    axs[0, 1].set_xlabel('Probability')\n",
    "    axs[0, 1].set_ylabel('Frequency')\n",
    "    axs[0,1].set_title('Probability Histogram')\n",
    "    axs[0,1].legend()\n",
    "\n",
    "    #ODDS Ratio\n",
    "\n",
    "    axs[1,0].bar(range(0,10),odds_train['Odds'])\n",
    "    axs[1,0].set_xlabel('Decil')\n",
    "    axs[1,0].set_ylabel('Odds')\n",
    "    axs[1,0].set_title('Odds Ratio')\n",
    "\n",
    "    #KS Chart\n",
    "\n",
    "    axs[1,1].plot(range(0,10),cumsum_event, label= 'Non Event',color='purple')\n",
    "    axs[1,1].plot(range(0,10),cumsum_no_event, label = 'Event',color='orange')\n",
    "    axs[1,1].set_xlabel('Decil')\n",
    "    axs[1,1].set_ylabel('Cumulative %')\n",
    "    axs[1,1].set_title('KS Chart')\n",
    "    axs[1,1].legend()  \n",
    "    \n",
    "    \n",
    "def metrics_regression(y_true,y_pred):\n",
    "    print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_true, y_pred))\n",
    "    print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_true, y_pred))\n",
    "    print('Root Mean Squared Error (RMSE):', metrics.mean_squared_error(y_true, y_pred, squared=False))\n",
    "    print('Mean Absolute Percentage Error (MAPE):', metrics.mean_absolute_percentage_error(y_true, y_pred))\n",
    "    print(\"Median Absolute Percentage Error(MdAPE):\", mdape(y_true,y_pred))\n",
    "    print(\"Volume Weighted Absolute Percentage Error(VWAPE):\",vwape(y_true, y_pred))\n",
    "    print('Explained Variance Score:', metrics.explained_variance_score(y_true, y_pred))\n",
    "    print('Max Error:', metrics.max_error(y_true, y_pred))\n",
    "    print('Median Absolute Error:', metrics.median_absolute_error(y_true, y_pred))\n",
    "    print('R^2:', metrics.r2_score(y_true, y_pred))\n",
    "    \n",
    "def mape(Y_actual,Y_Predicted):\n",
    "    mape = np.mean(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "    \n",
    "def mdape(Y_actual,Y_Predicted):\n",
    "    mape = np.median(np.abs((Y_actual - Y_Predicted)/Y_actual))*100\n",
    "    return mape\n",
    "\n",
    "def vwape(y_true, y_pred):\n",
    "    \n",
    "    vwape_value = np.sum(np.abs(y_pred-y_true))/np.sum(y_true)\n",
    "    \n",
    "    return vwape_value\n",
    "\n",
    "##############################\n",
    "def uplift_stat(y, yhat, at=0.3):\n",
    "    uplift = lift(y, yhat, at=at)\n",
    "    return uplift\n",
    "\n",
    "def lift(y, yhat, at):\n",
    "    \"\"\"Calculate lift.\"\"\"\n",
    "    # assert is_binary(y_true)\n",
    "    # assert not is_binary(y_pred_proba)\n",
    "    percentages, gains = cumulative_gain_curve(y, yhat, pos_label=1)\n",
    "    percentages = percentages[1:]\n",
    "    gains = gains[1:]\n",
    "    uplift = gains / percentages\n",
    "    n = len(gains)\n",
    "    uplift_at = uplift[int(n * at)]\n",
    "    return uplift_at\n",
    "\n",
    "def cumulative_gain_curve(\n",
    "    y_true, y_score, pos_label: str = None\n",
    "):# -> Tuple[np.array, np.array]\n",
    "    \"\"\"Generate the points necessary to plot the Cumulative Gain.\n",
    "    Note: This implementation is restricted to the binary classification task.\n",
    "    Args:\n",
    "        y_true (array-like, shape (n_samples)): True labels of the data.\n",
    "        y_score (array-like, shape (n_samples)): Target scores, can either be\n",
    "            probability estimates of the positive class, confidence values, or\n",
    "            non-thresholded measure of decisions (as returned by\n",
    "            decision_function on some classifiers).\n",
    "        pos_label (int or str, default=None): Label considered as positive and\n",
    "            others are considered negative\n",
    "    Returns:\n",
    "        percentages (numpy.ndarray): An array containing the X-axis values for\n",
    "            plotting the Cumulative Gains chart.\n",
    "        gains (numpy.ndarray): An array containing the Y-axis values for one\n",
    "            curve of the Cumulative Gains chart.\n",
    "    Raises:\n",
    "        ValueError: If `y_true` is not composed of 2 classes. The Cumulative\n",
    "            Gain Chart is only relevant in binary classification.\n",
    "    \"\"\"\n",
    "    y_true, y_score = np.asarray(y_true), np.asarray(y_score)\n",
    "    # ensure binary classification if pos_label is not specified\n",
    "    classes = np.unique(y_true)\n",
    "    if pos_label is None and not (\n",
    "        np.array_equal(classes, [0, 1])\n",
    "        or np.array_equal(classes, [-1, 1])\n",
    "        or np.array_equal(classes, [0])\n",
    "        or np.array_equal(classes, [-1])\n",
    "        or np.array_equal(classes, [1])\n",
    "    ):\n",
    "        raise ValueError(\"Data is not binary and pos_label is not specified\")\n",
    "    elif pos_label is None:\n",
    "        pos_label = 1.0\n",
    "    # make y_true a boolean vector\n",
    "    y_true = y_true == pos_label\n",
    "    sorted_indices = np.argsort(y_score)[::-1]\n",
    "    y_true = y_true[sorted_indices]\n",
    "    gains = np.cumsum(y_true)\n",
    "    percentages = np.arange(start=1, stop=len(y_true) + 1)\n",
    "    gains = gains / float(np.sum(y_true))\n",
    "    percentages = percentages / float(len(y_true))\n",
    "    gains = np.insert(gains, 0, [0])\n",
    "    percentages = np.insert(percentages, 0, [0])\n",
    "    return percentages, gains\n",
    "\n",
    "def ks_stat(y, yhat):\n",
    "    return ks_2samp(yhat[y==1], yhat[y!=1]).statistic\n",
    "\n",
    "\n",
    "#ks_scorer = make_scorer(ks_stat, needs_proba=True)\n",
    "\n",
    "#roc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "\n",
    "#log_scorer = make_scorer(log_loss, needs_proba=True, greater_is_better=False)\n",
    "\n",
    "#lift_scorer = make_scorer(uplift_stat, needs_proba=True, greater_is_better=True)\n",
    "\n",
    "def uplift_metric(y_true, prob_predict, percentile):\n",
    "    \"\"\" Calculate the performance of the model compared to random classification\n",
    "    Args:\n",
    "        y_true (list): True target variable\n",
    "        prob_predict (list): probability estimated by the model\n",
    "        percentile (int 1-100): take data sorted above \"n\" percentile\n",
    "        \n",
    "    Returns:\n",
    "        uplift (float): how many times the model performs better than random\n",
    "    \n",
    "    \"\"\"\n",
    "    thr_percentile = 100-percentile\n",
    "    \n",
    "    up_lift_df = pd.DataFrame()\n",
    "    up_lift_df[\"y_true\"] = y_true\n",
    "    up_lift_df[\"prob_predict\"] = prob_predict\n",
    "    \n",
    "    umbral = np.percentile(up_lift_df.prob_predict, thr_percentile)\n",
    "    \n",
    "    event_rate_up = up_lift_df[up_lift_df[\"prob_predict\"]>umbral].y_true.sum()/up_lift_df[up_lift_df[\"prob_predict\"]>umbral].y_true.count()\n",
    "    \n",
    "    event_rate = up_lift_df.y_true.sum()/up_lift_df.y_true.count()\n",
    "    \n",
    "    uplift = event_rate_up/event_rate\n",
    "    \n",
    "    return uplift\n",
    "\n",
    "def uplift_metric_substract(y_true, prob_predict, percentile):\n",
    "    \"\"\" Calculate the performance of the model compared to random classification\n",
    "    Args:\n",
    "        y_true (list): True target variable\n",
    "        prob_predict (list): probability estimated by the model\n",
    "        percentile (int 1-100): take data sorted above \"n\" percentile\n",
    "        \n",
    "    Returns:\n",
    "        uplift (float): what is the increase on the event rate compared to random classification\n",
    "    \n",
    "    \"\"\"\n",
    "    thr_percentile = 100-percentile\n",
    "    \n",
    "    up_lift_df = pd.DataFrame()\n",
    "    up_lift_df[\"y_true\"] = y_true\n",
    "    up_lift_df[\"prob_predict\"] = prob_predict\n",
    "    \n",
    "    umbral = np.percentile(up_lift_df.prob_predict, thr_percentile)\n",
    "    \n",
    "    event_rate_up = up_lift_df[up_lift_df[\"prob_predict\"]>umbral].y_true.sum()/up_lift_df[up_lift_df[\"prob_predict\"]>umbral].y_true.count()\n",
    "    \n",
    "    event_rate = up_lift_df.y_true.sum()/up_lift_df.y_true.count()\n",
    "    \n",
    "    uplift_ = event_rate_up-event_rate\n",
    "    \n",
    "    return uplift_\n",
    "\n",
    "def event_rate(y_true, y_pred):\n",
    "    \"\"\" Plot the event rate of each decile after running the model.\n",
    "    y_true (list): True target variable\n",
    "    y_pred (list): probability estimated by the model\n",
    "        \n",
    "    Returns:\n",
    "        None, just plot the graph\n",
    "    \n",
    "    \"\"\"\n",
    "    df_event_rate = pd.DataFrame()\n",
    "    df_event_rate[\"score\"] = y_pred\n",
    "    df_event_rate[\"target\"] = y_true\n",
    "    \n",
    "    event_rate_pob = np.sum(y_true)/len(y_true)\n",
    "    event_table = cs.tabla_resumen(df_event_rate, \"score\", \"target\", division=10)\n",
    "    \n",
    "    plt.bar([str(i*10) for i in range(1,11)], event_table.sort_values(\"max_score\", ascending=False).per_events, color=\"purple\")\n",
    "    plt.axhline(y = event_rate_pob, color = 'r', linestyle = 'dashed')\n",
    "    plt.title(\"Event Rate Plot\")\n",
    "    plt.xlabel(\"Decile\")\n",
    "    plt.ylabel(\"Event Rate\")\n",
    "    \n",
    "def up_lift_plot(y_true, y_pred):\n",
    "    \n",
    "    \"\"\" Plot the uplift of each decile\n",
    "    y_true (list): True target variable\n",
    "    y_pred (list): probability estimated by the model\n",
    "        \n",
    "    Returns:\n",
    "        None, just plot the graph\n",
    "    \"\"\"\n",
    "    \n",
    "    up_lift_value = []\n",
    "    decile = []\n",
    "    for i in range(1, 11):\n",
    "        up_lift_value.append(uplift_metric(y_true, y_pred, i*10))\n",
    "        decile.append(str(i*10))\n",
    "    plt.bar(decile, up_lift_value, color=\"purple\")\n",
    "    plt.title(\"Uplift Plot\")\n",
    "    plt.xlabel(\"Decile\")\n",
    "    plt.ylabel(\"Uplift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1d48b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# mei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split_train_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76a1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "    \n",
    "def split_train_test(master_model_005_v1):\n",
    "    \n",
    "    df = master_model_005_v1\n",
    "    \n",
    "    df_num_var = [\"DEMO_egresos\",\n",
    "                  \"EQUIFAX_ncon_1_12\",\n",
    "                  \"flag_tenencia_r22\",\n",
    "                  \"R04_mean_plazo_12_meses\",\n",
    "                  \"AHO_MON_mean_saldo_12_meses\",\n",
    "                  \"R04_mean_monto_original_12_meses\",\n",
    "                  \"EQUIFAX_b_cant_vencido_36_12\",\n",
    "                  \"EQUIFAX_b_m_sal_odif6_12\",\n",
    "                  \"TRX_ingreso_median_monto_1_meses\",\n",
    "                 ]\n",
    "    \n",
    "    df_cat_var_demo = [\"DEMO_oficio\",\n",
    "                       \"DEMO_tipo_dependencia_sri\",\n",
    "                       'DEMO_estado_civil_rc',\n",
    "                      ]\n",
    "\n",
    "    var = df_num_var + df_cat_var_demo\n",
    "    \n",
    "    df = _prepare_mdt_model_005_v1(df, df_num_var, df_cat_var_demo)\n",
    "    \n",
    "    data_train = df.randomSplit([0.8, 0.2], seed=8)[0].withColumn(\"periodo\", F.lit(\"TRAIN\"))\n",
    "    data_test = df.randomSplit([0.8, 0.2], seed=8)[1].withColumn(\"periodo\", F.lit(\"TEST\"))\n",
    "    data = data_train.union(data_test).toPandas()\n",
    "    \n",
    "    data = data[var + [\"target\", \"periodo\"]]\n",
    "\n",
    "    data_train = data[data.periodo == \"TRAIN\"]\n",
    "    x_train = data_train[var]\n",
    "    y_train = data_train[\"target\"].values\n",
    "\n",
    "    data_test = data[data.periodo == \"TEST\"]\n",
    "    x_test = data_test[var]\n",
    "    y_test = data_test[\"target\"].values\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, data\n",
    "\n",
    "def _prepare_mdt_model_005_v1(df, df_num_var, df_cat_var_demo):    \n",
    "    \n",
    "    KEY_TEMPORARY_COLUMNS = [\"yyyy_snapshot\", \"mm_snapshot\", \"codigo_cliente\"]\n",
    "    cat_demo_vars_null = [\"DEMO_nivel_educacion\",\n",
    "                          \"DEMO_actividad\",\n",
    "                          \"DEMO_oficio\", \n",
    "                          \"DEMO_sector_economico_sb\", \n",
    "                          \"DEMO_subsector_economico_sb\",\n",
    "                          \"DEMO_profesion\",\n",
    "                          \"DEMO_pais\",\n",
    "                          \"DEMO_ciudad_domicilio\",\n",
    "                          \"DEMO_provincia_domicilio\",\n",
    "                          \"DEMO_provincia_producto\",\n",
    "                          \"DEMO_ciudad_producto\",\n",
    "                          \"DEMO_zona_domicilio\",\n",
    "                          \"DEMO_zona_producto\",\n",
    "                          'DEMO_estado_civil_rc',\n",
    "                          'DEMO_des_nacionalidad_rc',\n",
    "                          'DEMO_des_sexo_rc',\n",
    "                          'DEMO_flag_dependencia',\n",
    "                          'DEMO_tipo_dependencia_sri',\n",
    "                         ]\n",
    "    \n",
    "    cat_equifax_vars_null = [\"EQUIFAX_decision_score\",\n",
    "                             \"EQUIFAX_provincia\",\n",
    "                             \"EQUIFAX_estado_civil\",\n",
    "                             \"EQUIFAX_genero\",\n",
    "                             \"EQUIFAX_titulo\",\n",
    "                             \"EQUIFAX_nivel\",\n",
    "                             \"EQUIFAX_relacion_laboral\",\n",
    "                             \"EQUIFAX_actividad_economica\",\n",
    "                             \"EQUIFAX_titulo_academico\",\n",
    "                             \"EQUIFAX_validacion_ingreso\",\n",
    "                             \"EQUIFAX_nivel_ingresos\",\n",
    "                             \"EQUIFAX_indicador_covid\",\n",
    "                             \"EQUIFAX_decision_matriz_dual\",\n",
    "                             \"EQUIFAX_valor_cartera_castigada_y_demanda_judicial_sce\",\n",
    "                             \"EQUIFAX_valor_inhabilitados\",\n",
    "                             \"EQUIFAX_decision_final\",\n",
    "                             \"EQUIFAX_mensaje_monto\",\n",
    "                             \"EQUIFAX_calificacion_r_para_establecimientos\",\n",
    "                             \"EQUIFAX_mayor_plazo_vencido_en_el_sistema_financiero_en_ultimos_12_meses\",\n",
    "                            ]\n",
    "    \n",
    "    cat_vars_null = cat_demo_vars_null + cat_equifax_vars_null\n",
    "    num_vars_null = list(set(df.columns) - set(KEY_TEMPORARY_COLUMNS) - set(cat_vars_null))\n",
    "    \n",
    "    var = cat_vars_null + num_vars_null\n",
    "    \n",
    "    df = (df\n",
    "          .withColumn(\"DEMO_oficio\",\n",
    "                      F.when(F.col(\"DEMO_oficio\")\n",
    "                             .isin(\"EMPLEADO\", \"EMPLEADO PUBLICO\", \"EMPLEADO JEFE\", \"GERENTE\",\n",
    "                                   \"COORDINADOR\", \"EMPLEADO ADMINISTRATIVO\", \"EJECUTIVO\"),\n",
    "                             \"SERVICIOS PROFESIONALES\")\n",
    "                      .otherwise(F.col(\"DEMO_oficio\"))\n",
    "                     )\n",
    "          .filter(~(F.col(\"DEMO_oficio\")\n",
    "                    .isin(\"PROFESOR UNIVERSITARIO\", \"MILITAR\", \"SUPERVISOR,SUPERINTENDENTE\",\n",
    "                          \"EMPLEADO BANCARIO\", \"CAJERA\", \"SECRETARIA\", \"SUBGERENTE,SUBDIRECTOR\",\n",
    "                          \"DESPACHADOR DE ADUANAS\", \"DIGITADOR EN SISTEMA\")))\n",
    "         )\n",
    "    \n",
    "    df = (df\n",
    "          .filter(((F.col(\"segmento_agd\") == \"AFLUENTE\") & (F.col(\"target\") >= 5000) & (F.col(\"target\") < 15000)) |\n",
    "                  ((F.col(\"segmento_agd\") == \"AFLUENTE MASIVO\") & (F.col(\"target\") >= 2000) & (F.col(\"target\") < 5000)) |\n",
    "                  ((F.col(\"segmento_agd\") == \"MASIVO\") & (F.col(\"target\") < 2000))\n",
    "                 )\n",
    "          .filter(F.col(\"target\") >= 465)\n",
    "         )\n",
    "    \n",
    "    df = _fill_na_none(df, \"OTRO\", subset=cat_vars_null)\n",
    "    df = _fill_na_none(df, 0, subset=num_vars_null)\n",
    "    \n",
    "    df = _transaccionabilidad_en_productos(df, cat_equifax_vars_null)\n",
    "    \n",
    "    df = (df\n",
    "          .filter(\"sum_AHO_MON != 0\")\n",
    "         ).persist()\n",
    "    \n",
    "    df = (df\n",
    "          .filter((F.col(\"target\") <= _get_cut_tail(df, \"target\", \"up\", 1)))\n",
    "         )\n",
    "    \n",
    "    df = (df\n",
    "          .filter((F.col(\"DEMO_egresos\") <= _get_cut_tail(df, \"DEMO_egresos\", \"up\", 1)))\n",
    "          .withColumn(\"diference\", F.col(\"target\") - F.col(\"DEMO_egresos\"))\n",
    "          .withColumn(\"diference_abs\", F.abs(F.col(\"target\") - F.col(\"DEMO_egresos\")))\n",
    "          .filter(\"diference_abs < 6000\")\n",
    "          .filter(\"diference > -2700\")\n",
    "         )\n",
    "    return df\n",
    "\n",
    "def _transaccionabilidad_en_productos(df, cat_equifax_vars_null):\n",
    "    cols_sum_TRX = [s for s in df.columns if (\"TRX_egreso\" in s) | (\"TRX_ingreso\" in s)]\n",
    "    cols_sum_CDP = [s for s in df.columns if \"CDP\" in s]\n",
    "    cols_sum_AHO_MON = [s for s in df.columns if \"AHO_MON\" in s]\n",
    "    cols_sum_TRX_TC = [s for s in df.columns if \"TRX_TC\" in s]\n",
    "    cols_sum_R04 = [s for s in df.columns if \"R04\" in s]\n",
    "    cols_sum_R22 = [s for s in df.columns if \"R22\" in s]\n",
    "    cols_sum_EQUIFAX = list(set([s for s in df.columns if \"EQUIFAX\" in s]) - set(cat_equifax_vars_null))\n",
    "    \n",
    "    df = (df\n",
    "          .withColumn(\"sum_TRX\", sum([F.col(c) for c in cols_sum_TRX]))\n",
    "          .withColumn(\"sum_CDP\", sum([F.col(c) for c in cols_sum_CDP]))\n",
    "          .withColumn(\"sum_AHO_MON\", sum([F.col(c) for c in cols_sum_AHO_MON]))\n",
    "          .withColumn(\"sum_TRX_TC\", sum([F.col(c) for c in cols_sum_TRX_TC]))\n",
    "          .withColumn(\"sum_R04\", sum([F.col(c) for c in cols_sum_R04]))\n",
    "          .withColumn(\"sum_R22\", sum([F.col(c) for c in cols_sum_R22]))\n",
    "          .withColumn(\"sum_EQUIFAX\", sum([F.col(c) for c in cols_sum_EQUIFAX]))     \n",
    "         )\n",
    "    return df\n",
    "\n",
    "def _fill_na_none(df, value, subset=None):\n",
    "    if subset is None:\n",
    "        columns_to_fill = df.columns\n",
    "    else:\n",
    "        columns_to_fill = subset\n",
    "    \n",
    "    df = (reduce(lambda reduce_df, \n",
    "                        col_name: (reduce_df\n",
    "                                   .withColumn(col_name,\n",
    "                                               F.when(F.col(col_name).isNull(), value)\n",
    "                                               .otherwise(F.col(col_name))\n",
    "                                              )\n",
    "                                  ),\n",
    "                        columns_to_fill,\n",
    "                        df\n",
    "                       )\n",
    "                )\n",
    "    return df\n",
    "\n",
    "def _get_cut_tail(df, var, limit, rango):\n",
    "    df_pd = df.toPandas()\n",
    "    rango_ = 100 - rango\n",
    "    lower = np.percentile(df_pd[var].dropna(), rango, interpolation = 'midpoint')\n",
    "    upper = np.percentile(df_pd[var].dropna(), rango_, interpolation = 'midpoint')\n",
    "    \n",
    "    if limit == \"up\":\n",
    "        result = upper\n",
    "    elif limit == \"low\":\n",
    "        result = lower\n",
    "        \n",
    "    return result\n",
    "\n",
    "def _outlier_boundaries(df, var, limit):\n",
    "    df_pd = df.toPandas()\n",
    "    q1 = np.percentile(df_pd[var].dropna(), 25, interpolation = 'midpoint')\n",
    "    q3 = np.percentile(df_pd[var].dropna(), 75, interpolation = 'midpoint')\n",
    "    iqr = q3 - q1\n",
    "    lower = q1 - 1.5*iqr\n",
    "    upper = q3 + 1.5*iqr\n",
    "    \n",
    "    if limit == \"up\":\n",
    "        result = upper\n",
    "    elif limit == \"low\":\n",
    "        result = lower\n",
    "    \n",
    "    return result\n",
    "\n",
    "def _get_corr_num(corr_df, list_corr_num, df_null):\n",
    "    corr_num_df = corr_df.merge(df_null, on='variable', how='left')\n",
    "    corr_num_df = corr_num_df.merge(list_corr_num, on='variable', how='left')\n",
    "    return corr_num_df\n",
    "\n",
    "def _get_corr_df(data_train):\n",
    "    corr_df = data_train.toPandas().corr()\n",
    "    corr_df.reset_index(inplace=True)\n",
    "    corr_df = corr_df.rename(columns={\"index\": \"variable\", \"target\": \"corr\"})\n",
    "    corr_df[\"corr_abs\"] = corr_df[\"corr\"].abs()\n",
    "    corr_df = corr_df[[\"variable\", \"corr\", \"corr_abs\"]]\n",
    "    return corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from risk_stream.data_science.risk_model.ml_models import get_model_and_params\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import (StandardScaler, OneHotEncoder)\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "\n",
    "def train_model_005_v1(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    MIN_FREQUENCY = 0.02\n",
    "\n",
    "    numeric_features = [\"DEMO_egresos\",\n",
    "                  \"EQUIFAX_ncon_1_12\",\n",
    "                  \"flag_tenencia_r22\",\n",
    "                  \"R04_mean_plazo_12_meses\",\n",
    "                  \"AHO_MON_mean_saldo_12_meses\",\n",
    "                  \"R04_mean_monto_original_12_meses\",\n",
    "                  \"EQUIFAX_b_cant_vencido_36_12\",\n",
    "                  \"EQUIFAX_b_m_sal_odif6_12\",\n",
    "                  \"TRX_ingreso_median_monto_1_meses\",\n",
    "                 ]\n",
    "    \n",
    "    categorical_features = [\"DEMO_oficio\",\n",
    "                       \"DEMO_tipo_dependencia_sri\",\n",
    "                       'DEMO_estado_civil_rc',\n",
    "                      ]\n",
    "    \n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"scaler\", StandardScaler())\n",
    "              ]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[(\"one_hot\", \n",
    "                OneHotEncoder(handle_unknown=\"infrequent_if_exist\", \n",
    "                              drop =\"first\", \n",
    "                              min_frequency=MIN_FREQUENCY\n",
    "                             ))\n",
    "              ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[(\"cat\", categorical_transformer, categorical_features),\n",
    "                      (\"num\", numeric_transformer, numeric_features)\n",
    "                     ]\n",
    "    )\n",
    "\n",
    "    clf_t = Pipeline(\n",
    "        steps=[(\"preprocessor\", preprocessor)]\n",
    "    )\n",
    "    \n",
    "    # train\n",
    "    clf_t.fit(x_train)\n",
    "    mat_train = clf_t.transform(x_train)\n",
    "    x_train_t = pd.DataFrame(mat_train, columns = clf_t.get_feature_names_out())\n",
    "\n",
    "    # # test\n",
    "    mat_test = clf_t.transform(x_test)\n",
    "    x_test_t = pd.DataFrame(mat_test, columns = clf_t.get_feature_names_out())\n",
    "    \n",
    "    ###################################    \n",
    "    mod = QuantReg(y_train, x_train_t)\n",
    "    res = mod.fit(q=0.4, max_iter=10000)\n",
    "    print(res.summary())\n",
    "    \n",
    "    return res, x_train_t, y_train, x_test_t, y_test, clf_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3160e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e790fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (make_scorer, \n",
    "                             r2_score, \n",
    "                             mean_absolute_error,\n",
    "                             mean_squared_error, \n",
    "                             mean_absolute_percentage_error)\n",
    "\n",
    "def evaluate_model_005_v1(model_005_v1, x_train_t, y_train, x_test_t, y_test):\n",
    "    \"\"\"Calculate the coefficient of determination and log the result.\n",
    "    Args:\n",
    "        regressor: Trained model.\n",
    "        X_test: Testing data of independent features.\n",
    "        y_test: Testing data for price.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics_train = _get_metrics(y_train, x_train_t, model_005_v1)\n",
    "    metrics_test = _get_metrics(y_test, x_test_t, model_005_v1)\n",
    "    \n",
    "    print(\"metrics_train\")    \n",
    "    print(metrics_train)\n",
    "    print(\"metrics_test\")    \n",
    "    print(metrics_test)\n",
    "    \n",
    "    return metrics_train, metrics_test\n",
    "\n",
    "def _get_metrics(y_true, X, model):\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    r2_score_ = r2_score(y_true, y_pred)\n",
    "    r2_score_adjust_ = r2_score_adjust(y_true, X, model)\n",
    "    mse_ = mean_squared_error(y_true, y_pred)\n",
    "    mae_ = mean_absolute_error(y_true, y_pred)\n",
    "    mape_ = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mdape_ = median_absolute_percentage_error(y_true, y_pred)\n",
    "    \n",
    "    metrics = {'MSE': mse_,\n",
    "               'MAE': mae_,\n",
    "               'MAPE': mape_,\n",
    "               'MDAPE': mdape_,\n",
    "               'R2': r2_score_,\n",
    "               'R2_adjust': r2_score_adjust_\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "def r2_score_adjust(y_true, X, model):\n",
    "    n = X.shape[0]\n",
    "    p = X.shape[1]\n",
    "    y_pred = model.predict(X)\n",
    "    r2_score_ = r2_score(y_true, y_pred)    \n",
    "    return 1-(1-r2_score_)*(n-1)/(n-p-1)\n",
    "\n",
    "def median_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.median(np.abs((y_true - y_pred)/y_true))\n",
    "\n",
    "######################################################\n",
    "def ks_metric(data, probability, real):\n",
    "    \"\"\"\n",
    "    Return ks metric of a data set\n",
    "    Data: data frame containing probability predicted dependent variable and real dependent variable\n",
    "    Probability: name of the column of probabilities\n",
    "    Real: name of the column of the real dependent variable\n",
    "    Event = 1\n",
    "    No event =0\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=probability, ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], 10) # Create deciles\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    cumsum = 0\n",
    "    ks_list = []\n",
    "    \n",
    "    list_cumPercentage = []\n",
    "    percentage = (grouped.count()[real]-grouped.sum()[real])/((grouped.count()[real]-grouped.sum()[real]).sum()) #calculate the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum = cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "        \n",
    "    cumsum_no_event = pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event = (grouped.sum()[real]/grouped.sum()[real].sum()).cumsum() #acumulative sum of the percentages of the events\n",
    "    ks = cumsum_no_event-cumsum_event\n",
    "    \n",
    "    return max(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f39c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split_train_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55015aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "    \n",
    "def split_train_test_model_016_v0(master_model_005_v1):\n",
    "    \n",
    "    mdt = master_model_005_v1\n",
    "    \n",
    "    mdt = mdt.where((mdt.segmento_agd==\"MASIVO\")|(mdt.segmento_agd==\"AFLUENTE MASIVO\")|(mdt.segmento_agd==\"AFLUENTE\"))\n",
    "\n",
    "    mdt = mdt.where(((mdt.segmento_agd==\"AFLUENTE\")&(mdt.target>=5000)&(mdt.target<15000))|\n",
    "                    ((mdt.segmento_agd==\"AFLUENTE MASIVO\")&(mdt.target>=2000)&(mdt.target<5000))\n",
    "                    |((mdt.segmento_agd==\"MASIVO\")&(mdt.target<2000)))\n",
    "\n",
    "    mdt = mdt.withColumn(\"periodo\", F.when((mdt.yyyy_snapshot==2022)&(mdt.mm_snapshot>=7), \"TEST\").otherwise(\"TRAIN\"))\n",
    " \n",
    "    df = mdt.toPandas()\n",
    "    \n",
    "    df = df[df.target>=465]\n",
    "    \n",
    "    df_train = df[df.periodo==\"TRAIN\"]\n",
    "    df_test = df[df.periodo==\"TEST\"]\n",
    "    \n",
    "    num_var = [\"EQUIFAX_ingreso\", \"EQUIFAX_score\", \n",
    "                \"EQUIFAX_ncon_1_24\", \"EQUIFAX_b_cant_x_vencer_3\", \"EQUIFAX_b_deuda_total_12\", \n",
    "                          ]\n",
    "\n",
    "    cat_var = []\n",
    "    \n",
    "    target = [\"target\"]\n",
    "    \n",
    "    data_train = pd.get_dummies(df_train[num_var+cat_var+target], drop_first=True)\n",
    "    data_test = pd.get_dummies(df_test[num_var+cat_var+target], drop_first=True) \n",
    "    \n",
    "    return data_train, data_test, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed76e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc505be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import risk_stream.utils.data_processing as dp\n",
    "\n",
    "\n",
    "\n",
    "def train_model_016_v0(data_train):\n",
    "    \n",
    "    df_columns = list(data_train.columns)\n",
    "    df_columns.remove(\"target\")\n",
    "    mod = smf.quantreg(dp.reg_qant(\"target\", df_columns), data_train)\n",
    "    model_016_v0 = mod.fit(q=0.21)\n",
    "    \n",
    "    return model_016_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2da4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from risk_stream.utils.metrics import mdape, mape\n",
    "\n",
    "\n",
    "def evaluate_model_016_v0(model_016_v0, data_train, data_test):\n",
    "    \n",
    "    df_columns = list(data_train.columns)\n",
    "    df_columns.remove(\"target\")\n",
    "    \n",
    "    mape_train = mape(data_train.target, model_016_v0.predict(data_train[df_columns]))\n",
    "    \n",
    "    mape_test = mape(data_test.target, model_016_v0.predict(data_test[df_columns]))\n",
    "    \n",
    "    mdape_train = mdape(data_train.target, model_016_v0.predict(data_train[df_columns]))\n",
    "    \n",
    "    mdape_test = mdape(data_test.target, model_016_v0.predict(data_test[df_columns]))\n",
    "    \n",
    "    _result_metric = {\n",
    "                      'mape_train': mape_train,\n",
    "                      'mape_test': mape_test,\n",
    "                      'mdape_train': mdape_train,\n",
    "                      \"mdape_test\": mdape_test\n",
    "                    }\n",
    "    \n",
    "    print(_result_metric)\n",
    "    \n",
    "    return _result_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde6b04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3cd296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split_train_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de738412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "    \n",
    "def split_train_test(master_model_003_v1):\n",
    "    \n",
    "    df = master_model_003_v1\n",
    "    \n",
    "    df_num_var = [\n",
    "    'EQUIFAX_ncon_1_24',\n",
    "    'EQUIFAX_score',\n",
    "    'EQUIFAX_b_cant_total_12',\n",
    "    'EQUIFAX_b_cant_odif36_vig_3',\n",
    "    'EQUIFAX_b_m_sal_odif36_3',\n",
    "    'AHO_MON_mean_saldo_3_meses',\n",
    "    'TRX_egreso_sum_monto_1_meses',\n",
    "    'TRX_egreso_cont_trx_12_meses',\n",
    "    'DEMO_activos',\n",
    "    'TRX_egreso_median_monto_12_meses', \n",
    "    'EQUIFAX_b_m_sal_odifs_3',\n",
    "    ]\n",
    "\n",
    "    df_cat_var = [\n",
    "        'DEMO_actividad',\n",
    "                 ]\n",
    "    \n",
    "    var = df_num_var + df_cat_var\n",
    "    \n",
    "    data_train = df.randomSplit([0.8, 0.2], seed=8)[0].withColumn(\"periodo\", F.lit(\"TRAIN\"))\n",
    "    data_test = df.randomSplit([0.8, 0.2], seed=8)[1].withColumn(\"periodo\", F.lit(\"TEST\"))\n",
    "    data = data_train.union(data_test).toPandas()\n",
    "    \n",
    "    data = data[var + [\"target\", \"periodo\"]]\n",
    "\n",
    "    data_train = data[data.periodo == \"TRAIN\"]\n",
    "    x_train = data_train[var]\n",
    "    y_train = data_train[\"target\"].values\n",
    "\n",
    "    data_test = data[data.periodo == \"TEST\"]\n",
    "    x_test = data_test[var]\n",
    "    y_test = data_test[\"target\"].values\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d35970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_model_003_v1(y_train, x_train):\n",
    "\n",
    "    df_num_var = [\n",
    "    'EQUIFAX_ncon_1_24',\n",
    "    'EQUIFAX_score',\n",
    "    'EQUIFAX_b_cant_total_12',\n",
    "    'EQUIFAX_b_cant_odif36_vig_3',\n",
    "    'EQUIFAX_b_m_sal_odif36_3',\n",
    "    'AHO_MON_mean_saldo_3_meses',\n",
    "    'TRX_egreso_sum_monto_1_meses',\n",
    "    'TRX_egreso_cont_trx_12_meses',\n",
    "    'DEMO_activos',\n",
    "    'TRX_egreso_median_monto_12_meses', \n",
    "    'EQUIFAX_b_m_sal_odifs_3',\n",
    "    ]\n",
    "\n",
    "    df_cat_var = [\n",
    "        'DEMO_actividad',\n",
    "                 ]\n",
    "    \n",
    "    var = df_num_var + df_cat_var\n",
    "    \n",
    "    selection_criteria = {\n",
    "        \"iv\": {\"min\": 0.01, \"max\": 1},\n",
    "        \"quality_score\": {\"min\": 0.01},\n",
    "    }\n",
    "\n",
    "\n",
    "    binning_fit_params = {\n",
    "        \"EQUIFAX_b_m_sal_odif36_3\": {\"max_n_bins\": 2},\n",
    "                          \"AHO_MON_mean_saldo_3_meses\": {\"max_n_bins\": 3},\n",
    "                          \"EQUIFAX_gasto_hogar\":{\"max_n_bins\": 3},\n",
    "                          \"TRX_egreso_sum_monto_1_meses\":{\"max_n_bins\": 2},\n",
    "                          \"TRX_egreso_cont_trx_12_meses\":{\"max_n_bins\": 2},\n",
    "                          \"DEMO_activos\":{\"max_n_bins\": 2},\n",
    "                          \"TRX_egreso_median_monto_12_meses\":{\"max_n_bins\": 2},\n",
    "                          \"EQUIFAX_b_m_sal_odifs_3\":{\"max_n_bins\": 2},\n",
    "                         }\n",
    "\n",
    "    binning_process = BinningProcess(var, max_n_bins=5, \n",
    "                                     max_pvalue=0.8,\n",
    "                                     selection_criteria=selection_criteria, \n",
    "                                     binning_fit_params=binning_fit_params\n",
    "                                    )\n",
    "\n",
    "    estimator = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "    scorecard = Scorecard(binning_process=binning_process, reverse_scorecard=True, intercept_based=True,\n",
    "                          estimator=estimator, scaling_method=\"min_max\",\n",
    "                          scaling_method_params={\"min\": 0, \"max\": 1000})\n",
    "    \n",
    "    scorecard.fit(x_train, y_train, show_digits=4)\n",
    "    \n",
    "    return scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64454788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38c0939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "def evaluate_model_003_v1(scorecard, y_train, x_train, y_test, x_test):\n",
    "    \"\"\"Calculate the coefficient of determination and log the result.\n",
    "    Args:\n",
    "        regressor: Trained model.\n",
    "        X_test: Testing data of independent features.\n",
    "        y_test: Testing data for price.\n",
    "    \"\"\"\n",
    "    \n",
    "    probability_train = scorecard.predict_proba(x_train)[:, 1]\n",
    "    probability_test = scorecard.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    df_train[\"prob_train\"] = probability_train\n",
    "    df_train[\"y_train\"] = y_train\n",
    "    \n",
    "    df_test = pd.DataFrame()\n",
    "    df_test[\"prob_test\"] = probability_test\n",
    "    df_test[\"y_test\"] = y_test\n",
    "    \n",
    "    ks_train = ks_metric(df_train, \"prob_train\", \"y_train\")\n",
    "    ks_test = ks_metric(df_test, \"prob_test\", \"y_test\")\n",
    "    \n",
    "    gini_train = (2*roc_auc_score(df_train.y_train, df_train.prob_train) -1)\n",
    "    gini_test = (2*roc_auc_score(df_test.y_test, df_test.prob_test) -1)\n",
    "    \n",
    "    _result_metric = {\n",
    "                      'ks_train': ks_train,\n",
    "                      'ks_test': ks_test,\n",
    "                      'gini_train': gini_train,\n",
    "                      \"gini_test\": gini_test\n",
    "                    }\n",
    "    \n",
    "    print(_result_metric)\n",
    "    \n",
    "    return _result_metric\n",
    "\n",
    "def ks_metric(data, probability, real):\n",
    "    \"\"\"\n",
    "    Return ks metric of a data set\n",
    "    Data: data frame containing probability predicted dependent variable and real dependent variable\n",
    "    Probability: name of the column of probabilities\n",
    "    Real: name of the column of the real dependent variable\n",
    "    Event = 1\n",
    "    No event =0\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=probability, ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], 10) # Create deciles\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    cumsum = 0\n",
    "    ks_list = []\n",
    "    \n",
    "    list_cumPercentage = []\n",
    "    percentage = (grouped.count()[real]-grouped.sum()[real])/((grouped.count()[real]-grouped.sum()[real]).sum()) #calculate the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum = cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "        \n",
    "    cumsum_no_event = pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event = (grouped.sum()[real]/grouped.sum()[real].sum()).cumsum() #acumulative sum of the percentages of the events\n",
    "    ks = cumsum_no_event-cumsum_event\n",
    "    \n",
    "    return max(ks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c817d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split_train_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "def split_train_test(master_propension_no_clientes):\n",
    "    \n",
    "    var = ['ncon_1_24', 'cant_total_12_total',\n",
    "           'ingreso_estimado', \"edad\",\n",
    "           'provincia', 'decision_score', 'titulo',\n",
    "           'estado_civil', 'ncon_1_6']\n",
    "    \n",
    "    data = master_propension_no_clientes.select(var + [\"target\", \"periodo\"]).toPandas()\n",
    "    \n",
    "    data_train = data[data.periodo == \"TRAIN\"]\n",
    "    x_train = data_train[var]\n",
    "    y_train = data_train[\"target\"].values\n",
    "\n",
    "    data_test = data[data.periodo == \"TEST\"]\n",
    "    x_test = data_test[var]\n",
    "    y_test = data_test[\"target\"].values\n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_model_propension_no_clientes(y_train, x_train):\n",
    "\n",
    "    var = ['ncon_1_24', 'cant_total_12_total',\n",
    "           'ingreso_estimado', \"edad\",\n",
    "           'provincia', 'decision_score', 'titulo',\n",
    "           'estado_civil', 'ncon_1_6']\n",
    "    \n",
    "    selection_criteria = {\n",
    "    \"iv\": {\"min\": 0.01, \"max\": 1},\n",
    "    \"quality_score\": {\"min\": 0.01},\n",
    "    }\n",
    "\n",
    "\n",
    "    binning_process = BinningProcess(var, max_n_bins=5, max_pvalue=0.8,\n",
    "                                     selection_criteria=selection_criteria)\n",
    "\n",
    "    estimator = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "    scorecard = Scorecard(binning_process=binning_process, reverse_scorecard=True, intercept_based=True,\n",
    "                          estimator=estimator, scaling_method=\"min_max\",\n",
    "                          scaling_method_params={\"min\": 0, \"max\": 1000})\n",
    "    \n",
    "    scorecard = scorecard.fit(x_train, y_train, show_digits=4)\n",
    "    \n",
    "    return scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe719fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2a3977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "def ks_metric(data, probability, real):\n",
    "    \"\"\"\n",
    "    Return ks metric of a data set\n",
    "    Data: data frame containing probability predicted dependent variable and real dependent variable\n",
    "    Probability: name of the column of probabilities\n",
    "    Real: name of the column of the real dependent variable\n",
    "    Event = 1\n",
    "    No event =0\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=probability, ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], 10) # Create deciles\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    cumsum = 0\n",
    "    ks_list = []\n",
    "    \n",
    "    list_cumPercentage = []\n",
    "    percentage = (grouped.count()[real]-grouped.sum()[real])/((grouped.count()[real]-grouped.sum()[real]).sum()) #calculate the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum = cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "        \n",
    "    cumsum_no_event = pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event = (grouped.sum()[real]/grouped.sum()[real].sum()).cumsum() #acumulative sum of the percentages of the events\n",
    "    ks = cumsum_no_event-cumsum_event\n",
    "    \n",
    "    return max(ks)\n",
    "\n",
    "def evaluate_model_propension_no_clientes(scorecard, y_train, x_train, y_test, x_test):\n",
    "    \"\"\"Calculate the coefficient of determination and log the result.\n",
    "    Args:\n",
    "        regressor: Trained model.\n",
    "        X_test: Testing data of independent features.\n",
    "        y_test: Testing data for price.\n",
    "    \"\"\"\n",
    "    \n",
    "    probability_train = scorecard.predict_proba(x_train)[:, 1]\n",
    "    probability_test = scorecard.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    df_train[\"prob_train\"] = probability_train\n",
    "    df_train[\"y_train\"] = y_train\n",
    "    \n",
    "    df_test = pd.DataFrame()\n",
    "    df_test[\"prob_test\"] = probability_test\n",
    "    df_test[\"y_test\"] = y_test\n",
    "    \n",
    "    ks_train = ks_metric(df_train, \"prob_train\", \"y_train\")\n",
    "    ks_test = ks_metric(df_test, \"prob_test\", \"y_test\")\n",
    "    \n",
    "    gini_train = (2*roc_auc_score(df_train.y_train, df_train.prob_train) -1)\n",
    "    gini_test = (2*roc_auc_score(df_test.y_test, df_test.prob_test) -1)\n",
    "    \n",
    "    _result_metric = {\n",
    "                      'ks_train': ks_train,\n",
    "                      'ks_test': ks_test,\n",
    "                      'gini_train': gini_train,\n",
    "                      \"gini_test\": gini_test\n",
    "                    }\n",
    "    \n",
    "    print(_result_metric)\n",
    "    \n",
    "    return _result_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b9ce8d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4be7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split_train_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8e6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "    \n",
    "def split_train_test_model_014_v0(master_model_014_v0):\n",
    "    \n",
    "    mdt = master_model_014_v0\n",
    "\n",
    "    mdt = mdt.withColumn(\"max_mora_12_consumo\", F.greatest(F.col(\"mes1_mora\"),\n",
    "                                                 F.col(\"mes2_mora\"),\n",
    "                                                 F.col(\"mes3_mora\"),\n",
    "                                                 F.col(\"mes4_mora\"),\n",
    "                                                 F.col(\"mes5_mora\"),\n",
    "                                                 F.col(\"mes6_mora\"),\n",
    "                                                 F.col(\"mes7_mora\"),\n",
    "                                                 F.col(\"mes8_mora\"),\n",
    "                                                 F.col(\"mes9_mora\"),\n",
    "                                                 F.col(\"mes10_mora\"),\n",
    "                                                 F.col(\"mes11_mora\"),\n",
    "                                                 F.col(\"mes12_mora\"),\n",
    "                                                ))\n",
    "\n",
    "    mdt = mdt.withColumn(\"max_mora_12_tc_consumo\", F.greatest(F.col(\"max_mora_12_consumo\"),\n",
    "                                                 F.col(\"max_dias_mora_12_meses_target_tc\"),\n",
    "                                                ))\n",
    "\n",
    "    mdt = mdt.withColumn(\"target_12\", F.when(mdt.max_mora_12_consumo==0, 0)\n",
    "                                      .when(mdt.max_mora_12_consumo>=60, 1)\n",
    "                                      .otherwise(2))\n",
    "\n",
    "\n",
    "    mdt_activo = mdt.where(mdt.activo_credito==\"ACTIVO\").where(mdt.target_12!=2)\n",
    "\n",
    "    credit_var = [\"prom_saldo_12_meses_tc\", \"prom_saldo_12_meses_cr\",\n",
    "                 \"prom_saldo_vencido_12_meses_tc\", \"prom_saldo_vencido_12_meses_cr\",\n",
    "                 \"prom_dias_mora_12_meses_tc\", \"prom_dias_mora_12_meses_cr\",\n",
    "                 \"max_dias_mora_12_meses_tc\", \"max_dias_mora_12_meses_cr\"]\n",
    "    \n",
    "    var = [\"calificacion_equifax\", \n",
    "       'max_dias_mora_12_meses_tc_cr',\n",
    "  'prom_saldo_6_meses_aho_mon',\n",
    "  'edad',\n",
    " 'b_deuda_vencido_1_12',\n",
    " 'ingreso_monto_trx',\n",
    " 'egreso_count_trx',\n",
    " 'b_m_sal_odif36_3',\n",
    "  \"estado_civil_rc\", \n",
    "       \"b_m_sal_ocor_vig_12\"\n",
    "      ]\n",
    "\n",
    "    mdt_activo = (mdt_activo.na.fill(0, subset=credit_var)\n",
    "                 .withColumn(\"prom_saldo_12_meses_tc_cr\",\n",
    "                             F.col(\"prom_saldo_12_meses_tc\")+F.col(\"prom_saldo_12_meses_cr\"))\n",
    "                 .withColumn(\"prom_saldo_x_vencer_12_meses_tc_cr\",\n",
    "                             F.col(\"prom_saldo_x_vencer_12_meses_tc\")+F.col(\"prom_saldo_x_vencer_12_meses_cr\"))\n",
    "                 .withColumn(\"prom_saldo_vencido_12_meses_tc_cr\",\n",
    "                             F.col(\"prom_saldo_vencido_12_meses_tc\")+F.col(\"prom_saldo_vencido_12_meses_cr\"))\n",
    "                 .withColumn(\"prom_saldo_no_devenga_12_meses_tc_cr\",\n",
    "                             F.col(\"prom_saldo_no_devenga_12_meses_tc\")+F.col(\"prom_saldo_no_devenga_12_meses_cr\"))\n",
    "                 .withColumn(\"sum_prom_dias_mora_12_meses_tc_cr\",\n",
    "                             F.col(\"prom_dias_mora_12_meses_tc\")+F.col(\"prom_dias_mora_12_meses_cr\"))\n",
    "                 .withColumn(\"max_dias_mora_12_meses_tc_cr\",\n",
    "                             F.greatest(F.col(\"max_dias_mora_12_meses_tc\"), F.col(\"max_dias_mora_12_meses_cr\")))\n",
    "                 ).toPandas()\n",
    "\n",
    "    mdt_activo[\"count_nulls\"] = mdt_activo[[\"mes0_mora\", \"mes1_mora\", \"mes2_mora\",\n",
    "       \"mes3_mora\", \"mes4_mora\", \"mes4_mora\",\n",
    "       \"mes5_mora\", \"mes6_mora\", \"mes7_mora\",\n",
    "       \"mes8_mora\", \"mes9_mora\", \"mes10_mora\",\n",
    "       \"mes11_mora\", \"mes12_mora\"]].isnull().sum(axis=1)\n",
    "    mdt_activo = mdt_activo[(mdt_activo.count_nulls==0)]\n",
    "    mdt_activo[\"periodo\"] = np.where((mdt_activo.concesion_year==2022)|((mdt_activo.concesion_year==2021) & (mdt_activo.concesion_month>2)), \"TEST\", \"TRAIN\")\n",
    "    mdt_activo = mdt_activo.dropna(subset=[\"calificacion_equifax\", \"prom_saldo_6_meses_aho_mon\"])\n",
    "\n",
    "\n",
    "    data_train = mdt_activo[mdt_activo.periodo == \"TRAIN\"]\n",
    "    x_train = data_train[var]\n",
    "    y_train = data_train[\"target_12\"].values\n",
    "\n",
    "    data_test = mdt_activo[mdt_activo.periodo == \"TEST\"]\n",
    "    x_test = data_test[var]\n",
    "    y_test = data_test[\"target_12\"].values\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, mdt_activo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfa95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_model_014_v0(y_train, x_train):\n",
    "    \n",
    "    var = [\"calificacion_equifax\", \n",
    "               'max_dias_mora_12_meses_tc_cr',\n",
    "          'prom_saldo_6_meses_aho_mon',\n",
    "          'edad',\n",
    "         'b_deuda_vencido_1_12',\n",
    "         'ingreso_monto_trx',\n",
    "         'egreso_count_trx',\n",
    "         'b_m_sal_odif36_3',\n",
    "          \"estado_civil_rc\", \n",
    "               \"b_m_sal_ocor_vig_12\"\n",
    "              ]\n",
    "\n",
    "    \n",
    "    selection_criteria = {\n",
    "        \"iv\": {\"min\": 0.0001, \"max\": 5},\n",
    "        \"quality_score\": {\"min\": 0.0001},\n",
    "    }\n",
    "\n",
    "    binning_fit_params ={\n",
    "        \"prom_saldo_6_meses_aho_mon\": {\"max_n_bins\": 3},\n",
    "                        \"edad\": {\"max_n_bins\": 2},\n",
    "                        \"ingreso_monto_trx\":{\"max_n_bins\": 2},\n",
    "                        \"b_m_sal_odif36_3\":{\"max_n_bins\": 2},\n",
    "                        \"egreso_count_trx\":{\"max_n_bins\": 2},\n",
    "        \"b_m_sal_ocor_vig_12\":{\"max_n_bins\": 2}\n",
    "\n",
    "    }\n",
    "\n",
    "    binning_process = BinningProcess(var, max_n_bins=5, \n",
    "                                     selection_criteria=selection_criteria, binning_fit_params=binning_fit_params)\n",
    "\n",
    "    estimator = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "    scorecard = Scorecard(binning_process=binning_process, intercept_based=True,\n",
    "                          estimator=estimator, scaling_method=\"min_max\",\n",
    "                          scaling_method_params={\"min\": 0, \"max\": 1000}, rounding=True)\n",
    "    \n",
    "    scorecard.fit(x_train, y_train, show_digits=4)\n",
    "    \n",
    "    return scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb922f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f304838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "def ks_metric(data, probability, real):\n",
    "    \"\"\"\n",
    "    Return ks metric of a data set\n",
    "    Data: data frame containing probability predicted dependent variable and real dependent variable\n",
    "    Probability: name of the column of probabilities\n",
    "    Real: name of the column of the real dependent variable\n",
    "    Event = 1\n",
    "    No event =0\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=probability, ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], 10) # Create deciles\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    cumsum = 0\n",
    "    ks_list = []\n",
    "    \n",
    "    list_cumPercentage = []\n",
    "    percentage = (grouped.count()[real]-grouped.sum()[real])/((grouped.count()[real]-grouped.sum()[real]).sum()) #calculate the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum = cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "        \n",
    "    cumsum_no_event = pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event = (grouped.sum()[real]/grouped.sum()[real].sum()).cumsum() #acumulative sum of the percentages of the events\n",
    "    ks = cumsum_no_event-cumsum_event\n",
    "    \n",
    "    return max(ks)\n",
    "\n",
    "def evaluate_model_014_v0(scorecard, y_train, x_train, y_test, x_test):\n",
    "    \"\"\"Calculate the coefficient of determination and log the result.\n",
    "    Args:\n",
    "        regressor: Trained model.\n",
    "        X_test: Testing data of independent features.\n",
    "        y_test: Testing data for price.\n",
    "    \"\"\"\n",
    "    \n",
    "    probability_train = scorecard.predict_proba(x_train)[:, 1]\n",
    "    probability_test = scorecard.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    df_train[\"prob_train\"] = probability_train\n",
    "    df_train[\"y_train\"] = y_train\n",
    "    \n",
    "    df_test = pd.DataFrame()\n",
    "    df_test[\"prob_test\"] = probability_test\n",
    "    df_test[\"y_test\"] = y_test\n",
    "    \n",
    "    ks_train = ks_metric(df_train, \"prob_train\", \"y_train\")\n",
    "    ks_test = ks_metric(df_test, \"prob_test\", \"y_test\")\n",
    "    \n",
    "    gini_train = (2*roc_auc_score(df_train.y_train, df_train.prob_train) -1)\n",
    "    gini_test = (2*roc_auc_score(df_test.y_test, df_test.prob_test) -1)\n",
    "    \n",
    "    _result_metric = {\n",
    "                      'ks_train': ks_train,\n",
    "                      'ks_test': ks_test,\n",
    "                      'gini_train': gini_train,\n",
    "                      \"gini_test\": gini_test\n",
    "                    }\n",
    "    \n",
    "    print(_result_metric)\n",
    "    \n",
    "    return _result_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b67fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split_train_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03ab304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "    \n",
    "def split_train_test_model_015_v0(master_model_014_v0):\n",
    "    \n",
    "    mdt = master_model_014_v0\n",
    "    \n",
    "    mdt = mdt.withColumn(\"max_mora_12_consumo\", F.greatest(F.col(\"mes1_mora\"),\n",
    "                                                 F.col(\"mes2_mora\"),\n",
    "                                                 F.col(\"mes3_mora\"),\n",
    "                                                 F.col(\"mes4_mora\"),\n",
    "                                                 F.col(\"mes5_mora\"),\n",
    "                                                 F.col(\"mes6_mora\"),\n",
    "                                                 F.col(\"mes7_mora\"),\n",
    "                                                 F.col(\"mes8_mora\"),\n",
    "                                                 F.col(\"mes9_mora\"),\n",
    "                                                 F.col(\"mes10_mora\"),\n",
    "                                                 F.col(\"mes11_mora\"),\n",
    "                                                 F.col(\"mes12_mora\"),\n",
    "                                                ))\n",
    "\n",
    "    mdt = mdt.withColumn(\"max_mora_12_tc_consumo\", F.greatest(F.col(\"max_mora_12_consumo\"),\n",
    "                                                 F.col(\"max_dias_mora_12_meses_target_tc\"),\n",
    "                                                ))\n",
    "\n",
    "    mdt = mdt.withColumn(\"target_12\", F.when(mdt.max_mora_12_consumo==0, 0)\n",
    "                                      .when(mdt.max_mora_12_consumo>=60, 1)\n",
    "                                      .otherwise(2))\n",
    "\n",
    "    mdt_activo_tc = mdt.where(mdt.activo_tc==\"ACTIVO\")\n",
    "\n",
    "    mdt_activo_tc = mdt_activo_tc.toPandas()\n",
    "    mdt_activo_tc[\"count_nulls\"] = mdt_activo_tc[[\"mes0_mora\", \"mes1_mora\", \"mes2_mora\",\n",
    "        \"mes3_mora\", \"mes4_mora\", \"mes4_mora\",\n",
    "        \"mes5_mora\", \"mes6_mora\", \"mes7_mora\",\n",
    "        \"mes8_mora\", \"mes9_mora\", \"mes10_mora\",\n",
    "        \"mes11_mora\", \"mes12_mora\"]].isnull().sum(axis=1)\n",
    "\n",
    "    mdt_activo_tc = mdt_activo_tc[(mdt_activo_tc.count_nulls==0)|(mdt_activo_tc.target_12==1)]\n",
    "\n",
    "    mdt_activo_tc[\"periodo\"] = np.where((mdt_activo_tc.concesion_year==2022)|((mdt_activo_tc.concesion_year==2021) & (mdt_activo_tc.concesion_month>2)), \"TEST\", \"TRAIN\")\n",
    "\n",
    "    mdt_activo_tc = mdt_activo_tc.dropna(subset=[\"b_m_sal_ocor_vig_12\"])\n",
    "\n",
    "    mdt_activo_tc = mdt_activo_tc[mdt_activo_tc.target_12!=2]\n",
    "    \n",
    "    var = [\"calificacion_equifax\", 'b_deuda_vencido_12',\n",
    "             'edad',\n",
    "             'max_meses_mora_12_meses_tc',\n",
    "             'b_m_sal_orot_vig_3',\n",
    "             'prom_saldo_3_meses_tc',\n",
    "             'b_m_sal_ocor_vig_3',\n",
    "             'b_cant_x_vencer_12',\n",
    "             'b_m_sal_odif6_3',\n",
    "             'b_cant_odif6_vig_3',\n",
    "              \"estado_civil_rc\"]\n",
    "\n",
    "    data_train = mdt_activo_tc[mdt_activo_tc.periodo == \"TRAIN\"]\n",
    "    x_train = data_train[var]\n",
    "    y_train = data_train[\"target_12\"].values\n",
    "\n",
    "    data_test = mdt_activo_tc[mdt_activo_tc.periodo == \"TEST\"]\n",
    "    x_test = data_test[var]\n",
    "    y_test = data_test[\"target_12\"].values\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, mdt_activo_tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a890798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_model_015_v0(y_train, x_train):\n",
    "    \n",
    "    var = [\"calificacion_equifax\", 'b_deuda_vencido_12',\n",
    "             'edad',\n",
    "             'max_meses_mora_12_meses_tc',\n",
    "             'b_m_sal_orot_vig_3',\n",
    "             'prom_saldo_3_meses_tc',\n",
    "             'b_m_sal_ocor_vig_3',\n",
    "             'b_cant_x_vencer_12',\n",
    "             'b_m_sal_odif6_3',\n",
    "             'b_cant_odif6_vig_3',\n",
    "              \"estado_civil_rc\"]\n",
    "\n",
    "    \n",
    "    selection_criteria = {\n",
    "        \"iv\": {\"min\": 0.0001, \"max\": 5},\n",
    "        \"quality_score\": {\"min\": 0.0001},\n",
    "    }\n",
    "\n",
    "    binning_fit_params ={\n",
    "            \"b_m_sal_odif6_3\": {\"max_n_bins\": 2},\n",
    "                            \"b_cant_x_vencer_12\": {\"max_n_bins\": 2},\n",
    "                            \"b_m_sal_ocor_vig_3\":{\"max_n_bins\": 2},\n",
    "                            \"prom_saldo_3_meses_tc\":{\"max_n_bins\": 2},\n",
    "                            \"edad\":{\"max_n_bins\": 2},\n",
    "            \"b_m_sal_orot_vig_3\":{\"max_n_bins\": 2},\n",
    "            \"b_cant_odif6_vig_3\":{\"max_n_bins\": 2},\n",
    "            \"b_deuda_vencido_12\":{\"max_n_bins\": 2}\n",
    "\n",
    "        }\n",
    "\n",
    "    binning_process = BinningProcess(var, max_n_bins=5, \n",
    "                                     selection_criteria=selection_criteria, binning_fit_params=binning_fit_params)\n",
    "\n",
    "    estimator = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "    scorecard = Scorecard(binning_process=binning_process, intercept_based=True,\n",
    "                          estimator=estimator, scaling_method=\"min_max\",\n",
    "                          scaling_method_params={\"min\": 0, \"max\": 1000}, rounding=True)\n",
    "\n",
    "    scorecard.fit(x_train, y_train, show_digits=4)\n",
    "    \n",
    "    return scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab11231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a791ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "def ks_metric(data, probability, real):\n",
    "    \"\"\"\n",
    "    Return ks metric of a data set\n",
    "    Data: data frame containing probability predicted dependent variable and real dependent variable\n",
    "    Probability: name of the column of probabilities\n",
    "    Real: name of the column of the real dependent variable\n",
    "    Event = 1\n",
    "    No event =0\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=probability, ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], 10) # Create deciles\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    cumsum = 0\n",
    "    ks_list = []\n",
    "    \n",
    "    list_cumPercentage = []\n",
    "    percentage = (grouped.count()[real]-grouped.sum()[real])/((grouped.count()[real]-grouped.sum()[real]).sum()) #calculate the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum = cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "        \n",
    "    cumsum_no_event = pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event = (grouped.sum()[real]/grouped.sum()[real].sum()).cumsum() #acumulative sum of the percentages of the events\n",
    "    ks = cumsum_no_event-cumsum_event\n",
    "    \n",
    "    return max(ks)\n",
    "\n",
    "def evaluate_model_015_v0(scorecard, y_train, x_train, y_test, x_test):\n",
    "    \"\"\"Calculate the coefficient of determination and log the result.\n",
    "    Args:\n",
    "        regressor: Trained model.\n",
    "        X_test: Testing data of independent features.\n",
    "        y_test: Testing data for price.\n",
    "    \"\"\"\n",
    "    \n",
    "    probability_train = scorecard.predict_proba(x_train)[:, 1]\n",
    "    probability_test = scorecard.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    df_train[\"prob_train\"] = probability_train\n",
    "    df_train[\"y_train\"] = y_train\n",
    "    \n",
    "    df_test = pd.DataFrame()\n",
    "    df_test[\"prob_test\"] = probability_test\n",
    "    df_test[\"y_test\"] = y_test\n",
    "    \n",
    "    ks_train = ks_metric(df_train, \"prob_train\", \"y_train\")\n",
    "    ks_test = ks_metric(df_test, \"prob_test\", \"y_test\")\n",
    "    \n",
    "    gini_train = (2*roc_auc_score(df_train.y_train, df_train.prob_train) -1)\n",
    "    gini_test = (2*roc_auc_score(df_test.y_test, df_test.prob_test) -1)\n",
    "    \n",
    "    _result_metric = {\n",
    "                      'ks_train': ks_train,\n",
    "                      'ks_test': ks_test,\n",
    "                      'gini_train': gini_train,\n",
    "                      \"gini_test\": gini_test\n",
    "                    }\n",
    "    \n",
    "    print(_result_metric)\n",
    "    \n",
    "    return _result_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aa5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split_train_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "    \n",
    "def split_train_test_model_011_v1(master_model_014_v0):\n",
    "    \n",
    "    mdt = master_model_014_v0\n",
    "\n",
    "    mdt = mdt.withColumn(\"max_mora_12_consumo\", F.greatest(F.col(\"mes1_mora\"),\n",
    "                                                 F.col(\"mes2_mora\"),\n",
    "                                                 F.col(\"mes3_mora\"),\n",
    "                                                 F.col(\"mes4_mora\"),\n",
    "                                                 F.col(\"mes5_mora\"),\n",
    "                                                 F.col(\"mes6_mora\"),\n",
    "                                                 F.col(\"mes7_mora\"),\n",
    "                                                 F.col(\"mes8_mora\"),\n",
    "                                                 F.col(\"mes9_mora\"),\n",
    "                                                 F.col(\"mes10_mora\"),\n",
    "                                                 F.col(\"mes11_mora\"),\n",
    "                                                 F.col(\"mes12_mora\"),\n",
    "                                                ))\n",
    "\n",
    "    mdt = mdt.withColumn(\"target_12\", F.when(mdt.max_mora_12_consumo==0, 0)\n",
    "                                      .when(mdt.max_mora_12_consumo>=60, 1)\n",
    "                                      .otherwise(2))\n",
    "\n",
    "    mdt_inactivo = mdt.where(mdt.activo_credito==\"INACTIVO\").where(mdt.activo_aho_mon==\"ACTIVO\").where(mdt.target_12!=2).toPandas()\n",
    "\n",
    "    mdt_inactivo[\"count_nulls\"] = mdt_inactivo[[\"mes0_mora\", \"mes1_mora\", \"mes2_mora\",\n",
    "       \"mes3_mora\", \"mes4_mora\", \"mes4_mora\",\n",
    "       \"mes5_mora\", \"mes6_mora\", \"mes7_mora\",\n",
    "       \"mes8_mora\", \"mes9_mora\", \"mes10_mora\",\n",
    "       \"mes11_mora\", \"mes12_mora\"]].isnull().sum(axis=1)\n",
    "    mdt_inactivo = mdt_inactivo[(mdt_inactivo.count_nulls==0)]\n",
    "    mdt_inactivo = mdt_inactivo.sort_values(\"max_mora_12_consumo\", ascending=False).drop_duplicates(subset=[\"codigo_cliente\"], keep='first')\n",
    "\n",
    "    mdt_inactivo[\"periodo\"] = np.where((mdt_inactivo.concesion_year==2022)|((mdt_inactivo.concesion_year==2021) & (mdt_inactivo.concesion_month>2)), \"TEST\", \"TRAIN\")\n",
    "    mdt_inactivo = mdt_inactivo.dropna(subset=[\"calificacion_equifax\", \"prom_saldo_12_meses_aho_mon\"])\n",
    "    mdt_inactivo = mdt_inactivo[mdt_inactivo.prom_saldo_12_meses_aho_mon>0]\n",
    "\n",
    "    var = [\"calificacion_equifax\",\n",
    "           'prom_saldo_12_meses_aho_mon',\n",
    "     'b_deuda_vencido_12',\n",
    "     'edad',\n",
    "     'egreso_monto_trx',\n",
    "     'b_deuda_no_devenga_12',\n",
    "     'ingreso_count_trx',\n",
    "     'b_cant_x_vencer_12',\n",
    "      \"estado_civil_rc\"]\n",
    "\n",
    "    data_train = mdt_inactivo[mdt_inactivo.periodo == \"TRAIN\"]\n",
    "    x_train = data_train[var]\n",
    "    y_train = data_train[\"target_12\"].values\n",
    "\n",
    "    data_test = mdt_inactivo[mdt_inactivo.periodo == \"TEST\"]\n",
    "    x_test = data_test[var]\n",
    "    y_test = data_test[\"target_12\"].values\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, mdt_inactivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110d56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "def train_model_011_v1(y_train, x_train):\n",
    "    \n",
    "    var = [\"calificacion_equifax\",\n",
    "           'prom_saldo_12_meses_aho_mon',\n",
    "     'b_deuda_vencido_12',\n",
    "     'edad',\n",
    "     'egreso_monto_trx',\n",
    "     'b_deuda_no_devenga_12',\n",
    "     'ingreso_count_trx',\n",
    "     'b_cant_x_vencer_12',\n",
    "      \"estado_civil_rc\"]\n",
    "\n",
    "    \n",
    "    selection_criteria = {\n",
    "    \"iv\": {\"min\": 0.0001, \"max\": 10},\n",
    "    \"quality_score\": {\"min\": 0.0001},\n",
    "    }\n",
    "\n",
    "    binning_fit_params ={\n",
    "        \"prom_saldo_12_meses_aho_mon\": {\"max_n_bins\": 2},\n",
    "        \"b_cant_x_vencer_12\": {\"max_n_bins\": 2},\n",
    "        \"egreso_monto_trx\": {\"max_n_bins\": 2},\n",
    "        \"ingreso_count_trx\": {\"max_n_bins\": 2},\n",
    "        \"edad\": {\"max_n_bins\": 3},\n",
    "        \"b_deuda_vencido_12\": {\"max_n_bins\": 3}\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "    binning_process = BinningProcess(var, max_n_bins=5, \n",
    "                                     selection_criteria=selection_criteria, binning_fit_params=binning_fit_params)\n",
    "\n",
    "    estimator = LogisticRegression(solver=\"lbfgs\")\n",
    "\n",
    "    scorecard = Scorecard(binning_process=binning_process, intercept_based=True,\n",
    "                          estimator=estimator, scaling_method=\"min_max\",\n",
    "                          scaling_method_params={\"min\": 0, \"max\": 1000}, rounding=True)\n",
    "    \n",
    "    scorecard.fit(x_train, y_train, show_digits=4)\n",
    "    \n",
    "    return scorecard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bddfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67420197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from optbinning import BinningProcess\n",
    "from optbinning import Scorecard\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "def ks_metric(data, probability, real):\n",
    "    \"\"\"\n",
    "    Return ks metric of a data set\n",
    "    Data: data frame containing probability predicted dependent variable and real dependent variable\n",
    "    Probability: name of the column of probabilities\n",
    "    Real: name of the column of the real dependent variable\n",
    "    Event = 1\n",
    "    No event =0\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=probability, ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], 10) # Create deciles\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    cumsum = 0\n",
    "    ks_list = []\n",
    "    \n",
    "    list_cumPercentage = []\n",
    "    percentage = (grouped.count()[real]-grouped.sum()[real])/((grouped.count()[real]-grouped.sum()[real]).sum()) #calculate the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum = cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "        \n",
    "    cumsum_no_event = pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event = (grouped.sum()[real]/grouped.sum()[real].sum()).cumsum() #acumulative sum of the percentages of the events\n",
    "    ks = cumsum_no_event-cumsum_event\n",
    "    \n",
    "    return max(ks)\n",
    "\n",
    "def evaluate_model_011_v1(scorecard, y_train, x_train, y_test, x_test):\n",
    "    \"\"\"Calculate the coefficient of determination and log the result.\n",
    "    Args:\n",
    "        regressor: Trained model.\n",
    "        X_test: Testing data of independent features.\n",
    "        y_test: Testing data for price.\n",
    "    \"\"\"\n",
    "    \n",
    "    probability_train = scorecard.predict_proba(x_train)[:, 1]\n",
    "    probability_test = scorecard.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    df_train[\"prob_train\"] = probability_train\n",
    "    df_train[\"y_train\"] = y_train\n",
    "    \n",
    "    df_test = pd.DataFrame()\n",
    "    df_test[\"prob_test\"] = probability_test\n",
    "    df_test[\"y_test\"] = y_test\n",
    "    \n",
    "    ks_train = ks_metric(df_train, \"prob_train\", \"y_train\")\n",
    "    ks_test = ks_metric(df_test, \"prob_test\", \"y_test\")\n",
    "    \n",
    "    gini_train = (2*roc_auc_score(df_train.y_train, df_train.prob_train) -1)\n",
    "    gini_test = (2*roc_auc_score(df_test.y_test, df_test.prob_test) -1)\n",
    "    \n",
    "    _result_metric = {\n",
    "                      'ks_train': ks_train,\n",
    "                      'ks_test': ks_test,\n",
    "                      'gini_train': gini_train,\n",
    "                      \"gini_test\": gini_test\n",
    "                    }\n",
    "    \n",
    "    print(_result_metric)\n",
    "    \n",
    "    return _result_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### split_train_test ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2505df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def variable_grupos(template, data):\n",
    "    \"\"\"\n",
    "    Realiza la categorización de variables a los datos de acuerdo a un template.\n",
    "    Template: \n",
    "    Data: DataFrame\n",
    "    Output: dataframe\n",
    "    \"\"\"\n",
    "    template_n = template[template.TipoVariable==\"Numerica\"]\n",
    "    for variable in template_n.Variable.unique():\n",
    "        data[variable+\"_V\"] = data[variable]\n",
    "        \n",
    "        try:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(template[(template.Variable==variable)&(template.Nulo==\"SI\")][\"Dummy\"].iloc[0])\n",
    "            #print(variable)\n",
    "        except:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(\"_null\")\n",
    "            \n",
    "#        if len(data[data[variable+\"_V\"]==\"_null\"])<0.1*len(data):\n",
    "#            data[variable+\"_V\"] = np.where(data[variable+\"_V\"]==\"_null\",\"_0\",data[variable+\"_V\"])\n",
    "\n",
    "        for i in range(len(template[template.Variable == variable] )):\n",
    "            data[variable+\"_V\"] = np.where((data[variable]>template[template.Variable == variable].Min.iloc[i])&(data[variable]<= template[template.Variable == variable].Max.iloc[i]),template[template.Variable == variable].Dummy.iloc[i],data[variable+\"_V\"])\n",
    "\n",
    "    template_c = template[template.TipoVariable==\"Categorica\"]\n",
    "    for variable in template_c.Variable.unique():\n",
    "        data[variable+\"_V\"] = data[variable]\n",
    "        try:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(template[(template.Variable==variable)&(template.Nulo==\"SI\")][\"Dummy\"].iloc[0])\n",
    "            print(variable)\n",
    "        except:\n",
    "            data[variable+\"_V\"] = data[variable].fillna(\"_null\")\n",
    "\n",
    "        y = []    \n",
    "        for i in range(len(template[template.Variable == variable] )):\n",
    "            try:\n",
    "                x = template[template.Variable==variable][\"Lista_Categoricas\"].iloc[i].split(\"; \")\n",
    "                y = y+x\n",
    "        \n",
    "                data[variable+\"_V\"] = np.where(np.isin(data[variable],x),template[template.Variable == variable].Dummy.iloc[i],data[variable+\"_V\"])\n",
    "            except:\n",
    "                pass\n",
    "        data[variable+\"_V\"] = np.where((np.isin(data[variable],y)==False)&(data[variable].isnull()==False),template[(template.Variable==variable)&(template.Otros == \"SI\")][\"Dummy\"],data[variable+\"_V\"]) \n",
    "    return data\n",
    "\n",
    "    \n",
    "def split_train_test(master_cs_orig_no_clientes):\n",
    "    \n",
    "    data = master_cs_orig_no_clientes.toPandas()\n",
    "    \n",
    "    template_json = {'0': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'decision_score',\n",
    "      'Dummy': '_0',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'Rechazar',\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '1': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'decision_score',\n",
    "      'Dummy': '_1',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'Analizar',\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': 'SI'},\n",
    "     '2': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'decision_score',\n",
    "      'Dummy': '_2',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'Aprobar A',\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '3': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'decision_score',\n",
    "      'Dummy': '_3',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'Aprobar AA',\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '4': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'decision_score',\n",
    "      'Dummy': '_4',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'Aprobar AAA',\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '5': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'estado_civil',\n",
    "      'Dummy': '_0',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'SOLTERO',\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '6': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'estado_civil',\n",
    "      'Dummy': '_1',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'CASADO; DIVORCIADO; VIUDO; UNIÓN LIBRE',\n",
    "      'Nulo': None,\n",
    "      'Otros': 'SI'},\n",
    "     '7': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'provincia_tse',\n",
    "      'Dummy': '_0',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'GUAYAS; ESMERALDAS; LOS RIOS; MANABI; PICHINCHA; STO DGO TSACHILAS',\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '8': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'provincia_tse',\n",
    "      'Dummy': '_1',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'CARCHI; CHIMBORAZO; COTOPAXI; EL ORO; IMBABURA; OTRO; TUNGURAHUA; AZUAY; LOJA; MORONA SANTIAGO; ZAMORA CHINCHIPE',\n",
    "      'Nulo': None,\n",
    "      'Otros': 'SI'},\n",
    "     '9': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'relacion_dependencia',\n",
    "      'Dummy': '_0',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'Dependiente; Independiente',\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': 'SI'},\n",
    "     '10': {'TipoVariable': 'Categorica',\n",
    "      'Variable': 'relacion_dependencia',\n",
    "      'Dummy': '_1',\n",
    "      'Min': None,\n",
    "      'Max': None,\n",
    "      'Lista_Categoricas': 'Dependiente con Actividad',\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '11': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'edad',\n",
    "      'Dummy': '_0',\n",
    "      'Min': -1e+61,\n",
    "      'Max': 24.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '12': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'edad',\n",
    "      'Dummy': '_1',\n",
    "      'Min': 24.0,\n",
    "      'Max': 1e+52,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '13': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_vencido_12_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+45,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '14': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_vencido_12_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+38,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '15': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'deuda_vencido_12_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+53,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '16': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'deuda_vencido_12_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+47,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '17': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_odifs_vig_3',\n",
    "      'Dummy': '_0',\n",
    "      'Min': -1e+31,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '18': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_odifs_vig_3',\n",
    "      'Dummy': '_1',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+52,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '19': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'm_sal_odifs_3',\n",
    "      'Dummy': '_0',\n",
    "      'Min': -1e+42,\n",
    "      'Max': 42.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '20': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'm_sal_odifs_3',\n",
    "      'Dummy': '_1',\n",
    "      'Min': 42.0,\n",
    "      'Max': 1e+45,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '21': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_ov_ven_vig_3_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+27,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '22': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_ov_ven_vig_3_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+32,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '23': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'm_salven_ov_3_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+48,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '24': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'm_salven_ov_3_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+37,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '25': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_odif12_vig_3',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 1.0,\n",
    "      'Max': 1e+41,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '26': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_odif12_vig_3',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+33,\n",
    "      'Max': 1.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '27': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'ingreso_estimado',\n",
    "      'Dummy': '_0',\n",
    "      'Min': -1e+40,\n",
    "      'Max': 528.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '28': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'ingreso_estimado',\n",
    "      'Dummy': '_1',\n",
    "      'Min': 528.0,\n",
    "      'Max': 1e+42,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '29': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'm_sal_orot_vig_12',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 4683.71,\n",
    "      'Max': 1e+46,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '30': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'm_sal_orot_vig_12',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+41,\n",
    "      'Max': 4683.71,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '31': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'm_sal_orot_ven_3',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 10.0,\n",
    "      'Max': 1e+45,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '32': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'm_sal_orot_ven_3',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+40,\n",
    "      'Max': 10.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '33': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_odif36_vig_12',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+44,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '34': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_odif36_vig_12',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+42,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '35': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_castigada_3_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+53,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '36': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_castigada_3_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+51,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '37': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'deuda_no_devenga_3_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+49,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '38': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'deuda_no_devenga_3_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+40,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '39': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'deuda_vencido_3_3_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 31.0,\n",
    "      'Max': 1e+55,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '40': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'deuda_vencido_3_3_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+38,\n",
    "      'Max': 31.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '41': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_vencido_1_3_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+49,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '42': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'cant_vencido_1_3_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+44,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '43': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'deuda_vencido_1_3_total',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 0.0,\n",
    "      'Max': 1e+50,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '44': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'deuda_vencido_1_3_total',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+45,\n",
    "      'Max': 0.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None},\n",
    "     '45': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'ncon_1_24',\n",
    "      'Dummy': '_0',\n",
    "      'Min': 19.0,\n",
    "      'Max': 1e+44,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': None,\n",
    "      'Otros': None},\n",
    "     '46': {'TipoVariable': 'Numerica',\n",
    "      'Variable': 'ncon_1_24',\n",
    "      'Dummy': '_1',\n",
    "      'Min': -1e+47,\n",
    "      'Max': 19.0,\n",
    "      'Lista_Categoricas': None,\n",
    "      'Nulo': 'SI',\n",
    "      'Otros': None}}\n",
    "\n",
    "    template = pd.DataFrame.from_dict(template_json, orient=\"index\")\n",
    "    \n",
    "    data2 = variable_grupos(template, data)\n",
    "    \n",
    "    data_model_007_v0 = data2\n",
    "    \n",
    "    var = [ 'decision_score_V', 'edad_V', 'm_sal_odifs_3_V',\n",
    "           'cant_odif12_vig_3_V', 'ingreso_estimado_V', 'm_sal_orot_vig_12_V',\n",
    "            'cant_odif36_vig_12_V', 'cant_castigada_3_total_V', 'deuda_no_devenga_3_total_V',\n",
    "           'cant_vencido_1_3_total_V', 'ncon_1_24_V', 'estado_civil_V','relacion_dependencia_V']\n",
    "    data2 = data2[var+[\"target\", \"periodo\"]]\n",
    "\n",
    "    data_train = data2[data2.periodo==\"TRAIN\"].drop(\"periodo\", axis=1)\n",
    "    data_test = data2[data2.periodo==\"TEST\"].drop(\"periodo\", axis=1)\n",
    "\n",
    "    data_train = pd.get_dummies(data_train, drop_first=True)\n",
    "    data_test = pd.get_dummies(data_test, drop_first=True) \n",
    "    \n",
    "    var_model = data_train.columns[1:]\n",
    "\n",
    "    endog_train = data_train.target\n",
    "    exog_train = sm.add_constant(data_train[var_model])\n",
    "\n",
    "    endog_test = data_test.target\n",
    "    exog_test = sm.add_constant(data_test[var_model])\n",
    "    \n",
    "    return endog_train, exog_train, endog_test, exog_test, data_model_007_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf844c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c25ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def train_model_cs_orig_no_clientes(endog_train, exog_train):\n",
    "\n",
    "    mod = sm.Logit(endog_train, exog_train)\n",
    "    model = mod.fit()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfe199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluate ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef71d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
    "\n",
    "def ks_metric(data, probability, real):\n",
    "    \"\"\"\n",
    "    Return ks metric of a data set\n",
    "    Data: data frame containing probability predicted dependent variable and real dependent variable\n",
    "    Probability: name of the column of probabilities\n",
    "    Real: name of the column of the real dependent variable\n",
    "    Event = 1\n",
    "    No event =0\n",
    "    \"\"\"\n",
    "    data = data.sort_values(by=probability, ascending=True).reset_index(drop = True) #Sort the Data Frame by Probability\n",
    "    data[\"Index\"] = data.index  #create a column of index\n",
    "    data['Decile'] = pd.qcut(data[\"Index\"], 10) # Create deciles\n",
    "    grouped = data.groupby('Decile', as_index = False) #divide the Data frame in deciles\n",
    "    \n",
    "    cumsum = 0\n",
    "    ks_list = []\n",
    "    \n",
    "    list_cumPercentage = []\n",
    "    percentage = (grouped.count()[real]-grouped.sum()[real])/((grouped.count()[real]-grouped.sum()[real]).sum()) #calculate the percentages of non events in the deciles\n",
    "    for i in percentage:\n",
    "        cumsum = cumsum+i\n",
    "        list_cumPercentage.append(cumsum)\n",
    "        \n",
    "    cumsum_no_event = pd.Series(list_cumPercentage) #acumulative sum of the percentages of the non events\n",
    "    cumsum_event = (grouped.sum()[real]/grouped.sum()[real].sum()).cumsum() #acumulative sum of the percentages of the events\n",
    "    ks = cumsum_no_event-cumsum_event\n",
    "    \n",
    "    return max(ks)\n",
    "\n",
    "def evaluate_model_cs_orig(model, endog_train, exog_train, endog_test, exog_test):\n",
    "    \"\"\"Calculate the coefficient of determination and log the result.\n",
    "    Args:\n",
    "        regressor: Trained model.\n",
    "        X_test: Testing data of independent features.\n",
    "        y_test: Testing data for price.\n",
    "    \"\"\"\n",
    "    probability_test = model.predict(exog_test)\n",
    "    probability_train = model.predict(exog_train)\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    df_train[\"prob_train\"] = probability_train\n",
    "    df_train[\"y_train\"] = endog_train\n",
    "    \n",
    "    df_test = pd.DataFrame()\n",
    "    df_test[\"prob_test\"] = probability_test\n",
    "    df_test[\"y_test\"] = endog_test\n",
    "    \n",
    "    ks_train = ks_metric(df_train, \"prob_train\", \"y_train\")\n",
    "    ks_test = ks_metric(df_test, \"prob_test\", \"y_test\")\n",
    "    \n",
    "    gini_train = (2*roc_auc_score(df_train.y_train, df_train.prob_train) -1)\n",
    "    gini_test = (2*roc_auc_score(df_test.y_test, df_test.prob_test) -1)\n",
    "    \n",
    "    _result_metric = {\n",
    "                      'ks_train': ks_train,\n",
    "                      'ks_test': ks_test,\n",
    "                      'gini_train': gini_train,\n",
    "                      \"gini_test\": gini_test\n",
    "                    }\n",
    "    \n",
    "    print(_result_metric)\n",
    "    \n",
    "    return _result_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a53056a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a50f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9151de50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
